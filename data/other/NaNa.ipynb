{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f2089f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110692, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train3.csv')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c87eed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Nationality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89811</th>\n",
       "      <td>Sasalak Haiprakhon</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64915</th>\n",
       "      <td>Hsin Ping</td>\n",
       "      <td>Taiwanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72147</th>\n",
       "      <td>Pyotra Krecheuski</td>\n",
       "      <td>Belarusian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109035</th>\n",
       "      <td>Mathews Phosa</td>\n",
       "      <td>Sotho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46349</th>\n",
       "      <td>Kristína Royová</td>\n",
       "      <td>Slovak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104211</th>\n",
       "      <td>Erdenebatyn Bekhbayar</td>\n",
       "      <td>Mongolian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54689</th>\n",
       "      <td>Park Sa-rang</td>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17767</th>\n",
       "      <td>Géza Gyimóthy</td>\n",
       "      <td>Hungarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89320</th>\n",
       "      <td>Mawin Maneewong</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32117</th>\n",
       "      <td>Víctor Carranza</td>\n",
       "      <td>Colombian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Names Nationality\n",
       "89811      Sasalak Haiprakhon        Thai\n",
       "64915               Hsin Ping   Taiwanese\n",
       "72147       Pyotra Krecheuski  Belarusian\n",
       "109035          Mathews Phosa       Sotho\n",
       "46349         Kristína Royová      Slovak\n",
       "...                       ...         ...\n",
       "104211  Erdenebatyn Bekhbayar   Mongolian\n",
       "54689            Park Sa-rang      Korean\n",
       "17767           Géza Gyimóthy   Hungarian\n",
       "89320         Mawin Maneewong        Thai\n",
       "32117         Víctor Carranza   Colombian\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8df0556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Encode labels in column 'Nationality'.\n",
    "df['Nationality']= label_encoder.fit_transform(df['Nationality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca28a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input\n",
    "X = df.Names\n",
    "# define labels\n",
    "y = df.Nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486852f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99622,), (99622,), (11070,), (11070,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# set aside 10% of the dataset for test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.1)\n",
    "# inspect resulting shapes\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e726de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUMPY\n",
    "\n",
    "from __future__ import print_function\n",
    "import inflect\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "_inflect = inflect.engine()\n",
    "_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')\n",
    "_decimal_number_re = re.compile(r'([0-9]+\\.[0-9]+)')\n",
    "_pounds_re = re.compile(r'£([0-9\\,]*[0-9]+)')\n",
    "_dollars_re = re.compile(r'\\$([0-9\\.\\,]*[0-9]+)')\n",
    "_ordinal_re = re.compile(r'[0-9]+(st|nd|rd|th)')\n",
    "_number_re = re.compile(r'[0-9]+')\n",
    "\n",
    "\n",
    "def _remove_commas(m):\n",
    "    return m.group(1).replace(',', '')\n",
    "\n",
    "\n",
    "def _expand_decimal_point(m):\n",
    "    return m.group(1).replace('.', ' point ')\n",
    "\n",
    "\n",
    "def _expand_dollars(m):\n",
    "    match = m.group(1)\n",
    "    parts = match.split('.')\n",
    "    if len(parts) > 2:\n",
    "        return match + ' dollars'    # Unexpected format\n",
    "    dollars = int(parts[0]) if parts[0] else 0\n",
    "    cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n",
    "    if dollars and cents:\n",
    "        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n",
    "        cent_unit = 'cent' if cents == 1 else 'cents'\n",
    "        return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n",
    "    elif dollars:\n",
    "        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n",
    "        return '%s %s' % (dollars, dollar_unit)\n",
    "    elif cents:\n",
    "        cent_unit = 'cent' if cents == 1 else 'cents'\n",
    "        return '%s %s' % (cents, cent_unit)\n",
    "    else:\n",
    "        return 'zero dollars'\n",
    "\n",
    "\n",
    "def _expand_ordinal(m):\n",
    "    return _inflect.number_to_words(m.group(0))\n",
    "\n",
    "\n",
    "def _expand_number(m):\n",
    "    num = int(m.group(0))\n",
    "    if num > 1000 and num < 3000:\n",
    "        if num == 2000:\n",
    "            return 'two thousand'\n",
    "        elif num > 2000 and num < 2010:\n",
    "            return 'two thousand ' + _inflect.number_to_words(num % 100)\n",
    "        elif num % 100 == 0:\n",
    "            return _inflect.number_to_words(num // 100) + ' hundred'\n",
    "        else:\n",
    "            return _inflect.number_to_words(num, andword='', zero='oh', group=2).replace(', ', ' ')\n",
    "    else:\n",
    "        return _inflect.number_to_words(num, andword='')\n",
    "\n",
    "\n",
    "def normalize_numbers(text):\n",
    "    text = re.sub(_comma_number_re, _remove_commas, text)\n",
    "    text = re.sub(_pounds_re, r'\\1 pounds', text)\n",
    "    text = re.sub(_dollars_re, _expand_dollars, text)\n",
    "    text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n",
    "    text = re.sub(_ordinal_re, _expand_ordinal, text)\n",
    "    text = re.sub(_number_re, _expand_number, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d63b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABSENT', 'AH1 B S AE1 N T', 'AE1 B S AH0 N T', 'V\\\\\\n']\n",
      "['ABSTRACT', 'AE0 B S T R AE1 K T', 'AE1 B S T R AE2 K T', 'V\\\\\\n']\n",
      "['ABSTRACTS', 'AE0 B S T R AE1 K T S', 'AE1 B S T R AE0 K T S', 'V\\\\\\n']\n",
      "['ABUSE', 'AH0 B Y UW1 Z', 'AH0 B Y UW1 S', 'V\\\\\\n']\n",
      "['ABUSES', 'AH0 B Y UW1 Z IH0 Z', 'AH0 B Y UW1 S IH0 Z', 'V\\\\\\n']\n",
      "['ACCENT', 'AH0 K S EH1 N T', 'AE1 K S EH2 N T', 'V\\\\\\n']\n",
      "['ACCENTS', 'AE1 K S EH0 N T S', 'AE1 K S EH0 N T S', 'V\\\\\\n']\n",
      "['ADDICT', 'AH0 D IH1 K T', 'AE1 D IH2 K T', 'V\\\\\\n']\n",
      "['ADDICTS', 'AH0 D IH1 K T S', 'AE1 D IH2 K T S', 'V\\\\\\n']\n",
      "['ADVOCATE', 'AE1 D V AH0 K EY2 T', 'AE1 D V AH0 K AH0 T', 'V\\\\\\n']\n",
      "['ADVOCATES', 'AE1 D V AH0 K EY2 T S', 'AE1 D V AH0 K AH0 T S', 'V\\\\\\n']\n",
      "['AFFECT', 'AH0 F EH1 K T', 'AE1 F EH0 K T', 'V\\\\\\n']\n",
      "['AFFECTS', 'AH0 F EH1 K T S', 'AE1 F EH0 K T S', 'V\\\\\\n']\n",
      "['AFFIX', 'AH0 F IH1 K S', 'AE1 F IH0 K S', 'V\\\\\\n']\n",
      "['AFFIXES', 'AH0 F IH1 K S IH0 Z', 'AE1 F IH0 K S IH0 Z', 'V\\\\\\n']\n",
      "['AGGLOMERATE', 'AH0 G L AA1 M ER0 EY2 T', 'AH0 G L AA1 M ER0 AH0 T', 'V\\\\\\n']\n",
      "['AGGREGATE', 'AE1 G R AH0 G EY0 T', 'AE1 G R AH0 G AH0 T', 'V\\\\\\n']\n",
      "['AGGREGATES', 'AE1 G R AH0 G EY2 T S', 'AE1 G R AH0 G IH0 T S', 'V\\\\\\n']\n",
      "['ALLIES', 'AH0 L AY1 Z', 'AE1 L AY0 Z', 'V\\\\\\n']\n",
      "['ALLOY', 'AH0 L OY1', 'AE1 L OY2', 'V\\\\\\n']\n",
      "['ALLOYS', 'AH0 L OY1 Z', 'AE1 L OY2 Z', 'V\\\\\\n']\n",
      "['ALLY', 'AH0 L AY1', 'AE1 L AY0', 'V\\\\\\n']\n",
      "['ALTERNATE', 'AO1 L T ER0 N EY2 T', 'AO0 L T ER1 N AH0 T', 'V\\\\\\n']\n",
      "['ANALYSES', 'AH0 N AE1 L IH0 S IY2 Z', 'AE1 N AH0 L AY0 Z IH2 Z', 'V\\\\\\n']\n",
      "['ANIMATE', 'AE1 N AH0 M EY2 T', 'AE1 N AH0 M AH0 T', 'V\\\\\\n']\n",
      "['ANNEX', 'AH0 N EH1 K S', 'AE1 N EH2 K S', 'V\\\\\\n']\n",
      "['ANNEXES', 'AH0 N EH1 K S IH0 Z', 'AE1 N EH2 K S IH0 Z', 'V\\\\\\n']\n",
      "['APPROPRIATE', 'AH0 P R OW1 P R IY0 EY2 T', 'AH0 P R OW1 P R IY0 AH0 T', 'V\\\\\\n']\n",
      "['APPROXIMATE', 'AH0 P R AA1 K S AH0 M EY2 T', 'AH0 P R AA1 K S AH0 M AH0 T', 'V\\\\\\n']\n",
      "['ARTICULATE', 'AA0 R T IH1 K Y AH0 L AH0 T', 'AA0 R T IH1 K Y AH0 L EY2 T', 'V\\\\\\n']\n",
      "['ASPIRATE', 'AE1 S P ER0 EY2 T', 'AE1 S P ER0 AH0 T', 'V\\\\\\n']\n",
      "['ASPIRATES', 'AE1 S P ER0 EY2 T S', 'AE1 S P ER0 AH0 T S', 'V\\\\\\n']\n",
      "['ASSOCIATE', 'AH0 S OW1 S IY0 EY2 T', 'AH0 S OW1 S IY0 AH0 T', 'V\\\\\\n']\n",
      "['ASSOCIATES', 'AH0 S OW1 S IY0 EY2 T S', 'AH0 S OW1 S IY0 AH0 T S', 'V\\\\\\n']\n",
      "['ATTRIBUTE', 'AH0 T R IH1 B Y UW2 T', 'AE1 T R IH0 B Y UW0 T', 'V\\\\\\n']\n",
      "['ATTRIBUTES', 'AH0 T R IH1 B Y UW2 T S', 'AE1 T R IH0 B Y UW0 T S', 'V\\\\\\n']\n",
      "['BATHS', 'B AE1 TH S', 'B AE1 DH Z', 'V\\\\\\n']\n",
      "['BLESSED', 'B L EH1 S IH0 D', 'B L EH1 S T', 'V\\\\\\n']\n",
      "['CERTIFICATE', 'S ER0 T IH1 F IH0 K AH0 T', 'S ER0 T IH1 F IH0 K EY2 T', 'V\\\\\\n']\n",
      "['CERTIFICATES', 'S ER0 T IH1 F IH0 K EY2 T S', 'S ER0 T IH1 F IH0 K AH0 T S', 'V\\\\\\n']\n",
      "['CLOSE', 'K L OW1 Z', 'K L OW1 S', 'V\\\\\\n']\n",
      "['CLOSER', 'K L OW1 Z ER0', 'K L OW1 S ER0', 'N\\\\\\n']\n",
      "['CLOSES', 'K L OW1 Z IH0 Z', 'K L OW1 S IH0 Z', 'V\\\\\\n']\n",
      "['COLLECT', 'K AH0 L EH1 K T', 'K AA1 L EH0 K T', 'V\\\\\\n']\n",
      "['COLLECTS', 'K AH0 L EH1 K T S', 'K AA1 L EH0 K T S', 'V\\\\\\n']\n",
      "['COMBAT', 'K AH0 M B AE1 T', 'K AA1 M B AE0 T', 'V\\\\\\n']\n",
      "['COMBATS', 'K AH0 M B AE1 T S', 'K AH1 M B AE0 T S', 'V\\\\\\n']\n",
      "['COMBINE', 'K AH0 M B AY1 N', 'K AA1 M B AY0 N', 'V\\\\\\n']\n",
      "['COMMUNE', 'K AH0 M Y UW1 N', 'K AA1 M Y UW0 N', 'V\\\\\\n']\n",
      "['COMMUNES', 'K AH0 M Y UW1 N Z', 'K AA1 M Y UW0 N Z', 'V\\\\\\n']\n",
      "['COMPACT', 'K AH0 M P AE1 K T', 'K AA1 M P AE0 K T', 'V\\\\\\n']\n",
      "['COMPACTS', 'K AH0 M P AE1 K T S', 'K AA1 M P AE0 K T S', 'V\\\\\\n']\n",
      "['COMPLEX', 'K AH0 M P L EH1 K S', ' K AA1 M P L EH0 K S', 'ADJ\\\\\\n']\n",
      "['COMPLIMENT', 'K AA1 M P L AH0 M EH0 N T', 'K AA1 M P L AH0 M AH0 N T', 'V\\\\\\n']\n",
      "['COMPLIMENTS', 'K AA1 M P L AH0 M EH0 N T S', 'K AA1 M P L AH0 M AH0 N T S', 'V\\\\\\n']\n",
      "['COMPOUND', 'K AH0 M P AW1 N D', 'K AA1 M P AW0 N D', 'V\\\\\\n']\n",
      "['COMPOUNDS', 'K AH0 M P AW1 N D Z', 'K AA1 M P AW0 N D Z', 'V\\\\\\n']\n",
      "['COMPRESS', 'K AH0 M P R EH1 S', 'K AA1 M P R EH0 S', 'V\\\\\\n']\n",
      "['COMPRESSES', 'K AH0 M P R EH1 S IH0 Z', 'K AA1 M P R EH0 S AH0 Z', 'V\\\\\\n']\n",
      "['CONCERT', 'K AH0 N S ER1 T', 'K AA1 N S ER0 T', 'V\\\\\\n']\n",
      "['CONCERTS', 'K AH0 N S ER1 T S', 'K AA1 N S ER0 T S', 'V\\\\\\n']\n",
      "['CONDUCT', 'K AA0 N D AH1 K T', 'K AA1 N D AH0 K T', 'V\\\\\\n']\n",
      "['CONFEDERATE', 'K AH0 N F EH1 D ER0 EY2 T', 'K AH0 N F EH1 D ER0 AH0 T', 'V\\\\\\n']\n",
      "['CONFEDERATES', 'K AH0 N F EH1 D ER0 EY2 T S', 'K AH0 N F EH1 D ER0 AH0 T S', 'V\\\\\\n']\n",
      "['CONFINES', 'K AH0 N F AY1 N Z', 'K AA1 N F AY2 N Z', 'V\\\\\\n']\n",
      "['CONFLICT', 'K AH0 N F L IH1 K T', 'K AA1 N F L IH0 K T', 'V\\\\\\n']\n",
      "['CONFLICTS', 'K AH0 N F L IH1 K T S', 'K AA1 N F L IH0 K T S', 'V\\\\\\n']\n",
      "['CONGLOMERATE', 'K AH0 N G L AA1 M ER0 EY2 T', 'K AH0 N G L AA1 M ER0 AH0 T', 'V\\\\\\n']\n",
      "['CONGLOMERATES', 'K AH0 N G L AA1 M ER0 EY2 T S', 'K AH0 N G L AA1 M ER0 AH0 T S', 'V\\\\\\n']\n",
      "['CONSCRIPT', 'K AH0 N S K R IH1 P T', 'K AA1 N S K R IH0 P T', 'V\\\\\\n']\n",
      "['CONSCRIPTS', 'K AH0 N S K R IH1 P T S', 'K AA1 N S K R IH0 P T S', 'V\\\\\\n']\n",
      "['CONSOLE', 'K AH0 N S OW1 L', 'K AA1 N S OW0 L', 'V\\\\\\n']\n",
      "['CONSOLES', 'K AH0 N S OW1 L Z', 'K AA1 N S OW0 L Z', 'V\\\\\\n']\n",
      "['CONSORT', 'K AH0 N S AO1 R T', 'K AA1 N S AO0 R T', 'V\\\\\\n']\n",
      "['CONSTRUCT', 'K AH0 N S T R AH1 K T', 'K AA1 N S T R AH0 K T', 'V\\\\\\n']\n",
      "['CONSTRUCTS', 'K AH0 N S T R AH1 K T S', 'K AA1 N S T R AH0 K T S', 'V\\\\\\n']\n",
      "['CONSUMMATE', 'K AA1 N S AH0 M EY2 T', 'K AA0 N S AH1 M AH0 T', 'V\\\\\\n']\n",
      "['CONTENT', 'K AA1 N T EH0 N T', 'K AH0 N T EH1 N T', 'N\\\\\\n']\n",
      "['CONTENTS', 'K AH0 N T EH1 N T S', 'K AA1 N T EH0 N T S', 'V\\\\\\n']\n",
      "['CONTEST', 'K AH0 N T EH1 S T', 'K AA1 N T EH0 S T', 'V\\\\\\n']\n",
      "['CONTESTS', 'K AH0 N T EH1 S T S', 'K AA1 N T EH0 S T S', 'V\\\\\\n']\n",
      "['CONTRACT', 'K AH0 N T R AE1 K T', 'K AA1 N T R AE2 K T', 'V\\\\\\n']\n",
      "['CONTRACTS', 'K AH0 N T R AE1 K T S', 'K AA1 N T R AE2 K T S', 'V\\\\\\n']\n",
      "['CONTRAST', 'K AH0 N T R AE1 S T', 'K AA1 N T R AE0 S T', 'V\\\\\\n']\n",
      "['CONTRASTS', 'K AH0 N T R AE1 S T S', 'K AA1 N T R AE0 S T S', 'V\\\\\\n']\n",
      "['CONVERSE', 'K AH0 N V ER1 S', 'K AA1 N V ER0 S', 'V\\\\\\n']\n",
      "['CONVERT', 'K AH0 N V ER1 T', 'K AA1 N V ER0 T', 'V\\\\\\n']\n",
      "['CONVERTS', 'K AH0 N V ER1 T S', 'K AA1 N V ER0 T S', 'V\\\\\\n']\n",
      "['CONVICT', 'K AH0 N V IH1 K T', 'K AA1 N V IH0 K T', 'V\\\\\\n']\n",
      "['CONVICTS', 'K AH0 N V IH1 K T S', 'K AA1 N V IH0 K T S', 'V\\\\\\n']\n",
      "['COORDINATE', 'K OW0 AO1 R D AH0 N EY2 T', 'K OW0 AO1 R D AH0 N AH0 T', 'V\\\\\\n']\n",
      "['COORDINATES', 'K OW0 AO1 R D AH0 N EY2 T S', 'K OW0 AO1 R D AH0 N AH0 T S', 'V\\\\\\n']\n",
      "['COUNTERBALANCE', 'K AW1 N T ER0 B AE2 L AH0 N S', 'K AW2 N T ER0 B AE1 L AH0 N S', 'V\\\\\\n']\n",
      "['COUNTERBALANCES', 'K AW2 N T ER0 B AE1 L AH0 N S IH0 Z', 'K AW1 N T ER0 B AE2 L AH0 N S IH0 Z', 'V\\\\\\n']\n",
      "['CRABBED', 'K R AE1 B D', 'K R AE1 B IH0 D', 'V\\\\\\n']\n",
      "['CROOKED', 'K R UH1 K T', 'K R UH1 K AH0 D', 'V\\\\\\n']\n",
      "['CURATE', 'K Y UH0 R AH1 T', 'K Y UH1 R AH0 T', 'V\\\\\\n']\n",
      "['CURSED', 'K ER1 S T', 'K ER1 S IH0 D', 'V\\\\\\n']\n",
      "['DECOY', 'D IY0 K OY1', 'D IY1 K OY0', 'V\\\\\\n']\n",
      "['DECOYS', 'D IY0 K OY1 Z', 'D IY1 K OY0 Z', 'V\\\\\\n']\n",
      "['DECREASE', 'D IH0 K R IY1 S', 'D IY1 K R IY2 S', 'V\\\\\\n']\n",
      "['DECREASES', 'D IH0 K R IY1 S IH0 Z', 'D IY1 K R IY2 S IH0 Z', 'V\\\\\\n']\n",
      "['DEFECT', 'D IH0 F EH1 K T', 'D IY1 F EH0 K T', 'V\\\\\\n']\n",
      "['DEFECTS', 'D IH0 F EH1 K T S', 'D IY1 F EH0 K T S', 'V\\\\\\n']\n",
      "['DEGENERATE', 'D IH0 JH EH1 N ER0 EY2 T', 'D IH0 JH EH1 N ER0 AH0 T', 'V\\\\\\n']\n",
      "['DEGENERATES', 'D IH0 JH EH1 N ER0 EY2 T S', 'D IH0 JH EH1 N ER0 AH0 T S', 'V\\\\\\n']\n",
      "['DELEGATE', 'D EH1 L AH0 G EY2 T', 'D EH1 L AH0 G AH0 T', 'V\\\\\\n']\n",
      "['DELEGATES', 'D EH1 L AH0 G EY2 T S', 'D EH1 L AH0 G AH0 T S', 'V\\\\\\n']\n",
      "['DELIBERATE', 'D IH0 L IH1 B ER0 EY2 T', 'D IH0 L IH1 B ER0 AH0 T', 'V\\\\\\n']\n",
      "['DESERT', 'D IH0 Z ER1 T', 'D EH1 Z ER0 T', 'V\\\\\\n']\n",
      "['DESERTS', 'D IH0 Z ER1 T S', 'D EH1 Z ER0 T S', 'V\\\\\\n']\n",
      "['DESOLATE', 'D EH1 S AH0 L EY2 T', 'D EH1 S AH0 L AH0 T', 'V\\\\\\n']\n",
      "['DIAGNOSES', 'D AY1 AH0 G N OW2 Z IY0 Z', 'D AY2 AH0 G N OW1 S IY0 Z', 'V\\\\\\n']\n",
      "['DICTATE', 'D IH0 K T EY1 T', 'D IH1 K T EY2 T', 'V\\\\\\n']\n",
      "['DICTATES', 'D IH0 K T EY1 T S', 'D IH1 K T EY2 T S', 'V\\\\\\n']\n",
      "['DIFFUSE', 'D IH0 F Y UW1 Z', 'D IH0 F Y UW1 S', 'V\\\\\\n']\n",
      "['DIGEST', 'D AY0 JH EH1 S T', 'D AY1 JH EH0 S T', 'V\\\\\\n']\n",
      "['DIGESTS', 'D AY2 JH EH1 S T S', 'D AY1 JH EH0 S T S', 'V\\\\\\n']\n",
      "['DISCARD', 'D IH0 S K AA1 R D', 'D IH1 S K AA0 R D', 'V\\\\\\n']\n",
      "['DISCARDS', 'D IH0 S K AA1 R D Z', 'D IH1 S K AA0 R D Z', 'V\\\\\\n']\n",
      "['DISCHARGE', 'D IH0 S CH AA1 R JH', 'D IH1 S CH AA2 R JH', 'V\\\\\\n']\n",
      "['DISCHARGES', 'D IH0 S CH AA1 R JH AH0 Z', 'D IH1 S CH AA2 R JH AH0 Z', 'V\\\\\\n']\n",
      "['DISCOUNT', 'D IH0 S K AW1 N T', 'D IH1 S K AW0 N T', 'V\\\\\\n']\n",
      "['DISCOUNTS', 'D IH0 S K AW1 N T S', 'D IH1 S K AW2 N T S', 'V\\\\\\n']\n",
      "['DISCOURSE', 'D IH0 S K AO1 R S', 'D IH1 S K AO0 R S', 'V\\\\\\n']\n",
      "['DISCOURSES', 'D IH0 S K AO1 R S IH0 Z', 'D IH1 S K AO0 R S IH0 Z', 'V\\\\\\n']\n",
      "['DOCUMENT', 'D AA1 K Y UW0 M EH0 N T', 'D AA1 K Y AH0 M AH0 N T', 'V\\\\\\n']\n",
      "['DOCUMENTS', 'D AA1 K Y UW0 M EH0 N T S', 'D AA1 K Y AH0 M AH0 N T S', 'V\\\\\\n']\n",
      "['DOGGED', 'D AO1 G IH0 D', 'D AO1 G D', 'V\\\\\\n']\n",
      "['DUPLICATE', 'D UW1 P L AH0 K EY2 T', 'D UW1 P L AH0 K AH0 T', 'V\\\\\\n']\n",
      "['DUPLICATES', 'D UW1 P L AH0 K EY2 T S', 'D UW1 P L AH0 K AH0 T S', 'V\\\\\\n']\n",
      "['EJACULATE', 'IH0 JH AE1 K Y UW0 L EY2 T', 'IH0 JH AE1 K Y UW0 L AH0 T', 'V\\\\\\n']\n",
      "['EJACULATES', 'IH0 JH AE1 K Y UW0 L EY2 T S', 'IH0 JH AE1 K Y UW0 L AH0 T S', 'V\\\\\\n']\n",
      "['ELABORATE', 'IH0 L AE1 B ER0 EY2 T', 'IH0 L AE1 B R AH0 T', 'V\\\\\\n']\n",
      "['ENTRANCE', 'IH0 N T R AH1 N S', 'EH1 N T R AH0 N S', 'V\\\\\\n']\n",
      "['ENTRANCES', 'IH0 N T R AH1 N S AH0 Z', 'EH1 N T R AH0 N S AH0 Z', 'V\\\\\\n']\n",
      "['ENVELOPE', 'IH0 N V EH1 L AH0 P', 'EH1 N V AH0 L OW2 P', 'V\\\\\\n']\n",
      "['ENVELOPES', 'IH0 N V EH1 L AH0 P S', 'EH1 N V AH0 L OW2 P S', 'V\\\\\\n']\n",
      "['ESCORT', 'EH0 S K AO1 R T', 'EH1 S K AO0 R T', 'V\\\\\\n']\n",
      "['ESCORTS', 'EH0 S K AO1 R T S', 'EH1 S K AO0 R T S', 'V\\\\\\n']\n",
      "['ESSAY', 'EH0 S EY1', 'EH1 S EY2', 'V\\\\\\n']\n",
      "['ESSAYS', 'EH0 S EY1 Z', 'EH1 S EY2 Z', 'V\\\\\\n']\n",
      "['ESTIMATE', 'EH1 S T AH0 M EY2 T', 'EH1 S T AH0 M AH0 T', 'V\\\\\\n']\n",
      "['ESTIMATES', 'EH1 S T AH0 M EY2 T S', 'EH1 S T AH0 M AH0 T S', 'V\\\\\\n']\n",
      "['EXCESS', 'IH0 K S EH1 S', 'EH1 K S EH2 S', 'V\\\\\\n']\n",
      "['EXCISE', 'EH0 K S AY1 S', 'EH1 K S AY0 Z', 'V\\\\\\n']\n",
      "['EXCUSE', 'IH0 K S K Y UW1 Z', 'IH0 K S K Y UW1 S', 'V\\\\\\n']\n",
      "['EXCUSES', 'IH0 K S K Y UW1 Z IH0 Z', 'IH0 K S K Y UW1 S IH0 Z', 'V\\\\\\n']\n",
      "['EXPATRIATE', 'EH0 K S P EY1 T R IY0 EY2 T', 'EH0 K S P EY1 T R IY0 AH0 T', 'V\\\\\\n']\n",
      "['EXPATRIATES', 'EH0 K S P EY1 T R IY0 EY2 T S', 'EH0 K S P EY1 T R IY0 AH0 T S', 'V\\\\\\n']\n",
      "['EXPLOIT', 'EH1 K S P L OY2 T', 'EH2 K S P L OY1 T', 'V\\\\\\n']\n",
      "['EXPLOITS', 'EH1 K S P L OY2 T S', 'EH2 K S P L OY1 T S', 'V\\\\\\n']\n",
      "['EXPORT', 'IH0 K S P AO1 R T', 'EH1 K S P AO0 R T', 'V\\\\\\n']\n",
      "['EXPORTS', 'IH0 K S P AO1 R T S', 'EH1 K S P AO0 R T S', 'V\\\\\\n']\n",
      "['EXTRACT', 'IH0 K S T R AE1 K T', 'EH1 K S T R AE2 K T', 'V\\\\\\n']\n",
      "['EXTRACTS', 'IH0 K S T R AE1 K T S', 'EH1 K S T R AE2 K T S', 'V\\\\\\n']\n",
      "['FERMENT', 'F ER0 M EH1 N T', 'F ER1 M EH0 N T', 'V\\\\\\n']\n",
      "['FERMENTS', 'F ER0 M EH1 N T S', 'F ER1 M EH0 N T S', 'V\\\\\\n']\n",
      "['FRAGMENT', 'F R AE1 G M AH0 N T', 'F R AE0 G M EH1 N T', 'V\\\\\\n']\n",
      "['FRAGMENTS', 'F R AE0 G M EH1 N T S', 'F R AE1 G M AH0 N T S', 'V\\\\\\n']\n",
      "['FREQUENT', 'F R IY1 K W EH2 N T', 'F R IY1 K W AH0 N T', 'V\\\\\\n']\n",
      "['GRADUATE', 'G R AE1 JH AH0 W EY2 T', 'G R AE1 JH AH0 W AH0 T', 'V\\\\\\n']\n",
      "['GRADUATES', 'G R AE1 JH AH0 W EY2 T S', 'G R AE1 JH AH0 W AH0 T S', 'V\\\\\\n']\n",
      "['HOUSE', 'HH AW1 Z', 'HH AW1 S', 'V\\\\\\n']\n",
      "['IMPACT', 'IH2 M P AE1 K T', 'IH1 M P AE0 K T', 'V\\\\\\n']\n",
      "['IMPACTS', 'IH2 M P AE1 K T S', 'IH1 M P AE0 K T S', 'V\\\\\\n']\n",
      "['IMPLANT', 'IH2 M P L AE1 N T', 'IH1 M P L AE2 N T', 'V\\\\\\n']\n",
      "['IMPLANTS', 'IH2 M P L AE1 N T S', 'IH1 M P L AE2 N T S', 'V\\\\\\n']\n",
      "['IMPLEMENT', 'IH1 M P L AH0 M EH0 N T', 'IH1 M P L AH0 M AH0 N T', 'V\\\\\\n']\n",
      "['IMPLEMENTS', 'IH1 M P L AH0 M EH0 N T S', 'IH1 M P L AH0 M AH0 N T S', 'V\\\\\\n']\n",
      "['IMPORT', 'IH2 M P AO1 R T', 'IH1 M P AO2 R T', 'V\\\\\\n']\n",
      "['IMPORTS', 'IH2 M P AO1 R T S', 'IH1 M P AO2 R T S', 'V\\\\\\n']\n",
      "['IMPRESS', 'IH0 M P R EH1 S', 'IH1 M P R EH0 S', 'V\\\\\\n']\n",
      "['IMPRINT', 'IH1 M P R IH0 N T', 'IH2 M P R IH1 N T', 'V\\\\\\n']\n",
      "['IMPRINTS', 'IH2 M P R IH1 N T S', 'IH1 M P R IH0 N T S', 'V\\\\\\n']\n",
      "['INCENSE', 'IH2 N S EH1 N S', 'IH1 N S EH2 N S', 'V\\\\\\n']\n",
      "['INCLINE', 'IH2 N K L AY1 N', 'IH1 N K L AY0 N', 'V\\\\\\n']\n",
      "['INCLINES', 'IH2 N K L AY1 N Z', 'IH1 N K L AY0 N Z', 'V\\\\\\n']\n",
      "['INCORPORATE', 'IH2 N K AO1 R P ER0 EY2 T', 'IH2 N K AO1 R P ER0 AH0 T', 'V\\\\\\n']\n",
      "['INCREASE', 'IH2 N K R IY1 S', 'IH1 N K R IY2 S', 'V\\\\\\n']\n",
      "['INCREASES', 'IH2 N K R IY1 S IH0 Z', 'IH1 N K R IY2 S IH0 Z', 'V\\\\\\n']\n",
      "['INDENT', 'IH2 N D EH1 N T', 'IH1 N D EH0 N T', 'V\\\\\\n']\n",
      "['INDENTS', 'IH2 N D EH1 N T S', 'IH1 N D EH0 N T S', 'V\\\\\\n']\n",
      "['INEBRIATE', 'IH2 N EH1 B R IY0 EY2 T', 'IH2 N EH1 B R IY0 AH0 T', 'V\\\\\\n']\n",
      "['INEBRIATES', 'IH2 N EH1 B R IY0 EY2 T S', 'IH2 N EH1 B R IY0 AH0 T S', 'V\\\\\\n']\n",
      "['INITIATE', 'IH2 N IH1 SH IY0 EY2 T', 'IH2 N IH1 SH IY0 AH0 T', 'V\\\\\\n']\n",
      "['INITIATES', 'IH2 N IH1 SH IY0 EY2 T S', 'IH2 N IH1 SH IY0 AH0 T S', 'V\\\\\\n']\n",
      "['INLAY', 'IH2 N L EY1', 'IH1 N L EY2', 'V\\\\\\n']\n",
      "['INLAYS', 'IH2 N L EY1 Z', 'IH1 N L EY2 Z', 'V\\\\\\n']\n",
      "['INSERT', 'IH2 N S ER1 T', 'IH1 N S ER2 T', 'V\\\\\\n']\n",
      "['INSERTS', 'IH2 N S ER1 T S', 'IH1 N S ER2 T S', 'V\\\\\\n']\n",
      "['INSET', 'IH2 N S EH1 T', 'IH1 N S EH2 T', 'V\\\\\\n']\n",
      "['INSETS', 'IH2 N S EH1 T S', 'IH1 N S EH2 T S', 'V\\\\\\n']\n",
      "['INSTINCT', 'IH2 N S T IH1 NG K T', 'IH1 N S T IH0 NG K T', 'V\\\\\\n']\n",
      "['INSULT', 'IH2 N S AH1 L T', 'IH1 N S AH2 L T', 'V\\\\\\n']\n",
      "['INSULTS', 'IH2 N S AH1 L T S', 'IH1 N S AH2 L T S', 'V\\\\\\n']\n",
      "['INTERCHANGE', 'IH2 T ER0 CH EY1 N JH', 'IH1 N T ER0 CH EY2 N JH', 'V\\\\\\n']\n",
      "['INTERCHANGES', 'IH2 T ER0 CH EY1 N JH IH0 Z', 'IH1 N T ER0 CH EY2 N JH IH0 Z', 'V\\\\\\n']\n",
      "['INTERDICT', 'IH2 N T ER0 D IH1 K T', 'IH1 N T ER0 D IH2 K T', 'V\\\\\\n']\n",
      "['INTERDICTS', 'IH2 N T ER0 D IH1 K T S', 'IH1 N T ER0 D IH2 K T S', 'V\\\\\\n']\n",
      "['INTERN', 'IH0 N T ER1 N', 'IH1 N T ER0 N', 'V\\\\\\n']\n",
      "['INTERNS', 'IH0 N T ER1 N Z', 'IH1 N T ER0 N Z', 'V\\\\\\n']\n",
      "['INTIMATE', 'IH1 N T IH0 M EY2 T', 'IH1 N T AH0 M AH0 T', 'V\\\\\\n']\n",
      "['INTIMATES', 'IH1 N T IH0 M EY2 T S', 'IH1 N T AH0 M AH0 T S', 'V\\\\\\n']\n",
      "['INTROVERT', 'IH2 N T R AO0 V ER1 T', 'IH1 N T R AO0 V ER2 T', 'V\\\\\\n']\n",
      "['INTROVERTS', 'IH2 N T R AO0 V ER1 T S', 'IH1 N T R AO0 V ER2 T S', 'V\\\\\\n']\n",
      "['INVERSE', 'IH1 N V ER0 S', 'IH2 N V ER1 S', 'V\\\\\\n']\n",
      "['INVITE', 'IH2 N V AY1 T', 'IH1 N V AY0 T', 'V\\\\\\n']\n",
      "['INVITES', 'IH2 N V AY1 T S', 'IH1 N V AY0 T S', 'V\\\\\\n']\n",
      "['JAGGED', 'JH AE1 G D', 'JH AE1 G IH0 D', 'V\\\\\\n']\n",
      "['LEARNED', 'L ER1 N IH0 D', 'L ER1 N D', 'V\\\\\\n']\n",
      "['LEGITIMATE', 'L AH0 JH IH1 T AH0 M EY2 T', 'L AH0 JH IH1 T AH0 M AH0 T', 'V\\\\\\n']\n",
      "['MANDATE', 'M AE1 N D EY2 T', 'M AE2 N D EY1 T', 'V\\\\\\n']\n",
      "['MISCONDUCT', 'M IH2 S K AA1 N D AH0 K T', 'M IH2 S K AA0 N D AH1 K T', 'V\\\\\\n']\n",
      "['MISPRINT', 'M IH2 S P R IH1 N T', 'M IH1 S P R IH0 N T', 'V\\\\\\n']\n",
      "['MISPRINTS', 'M IH2 S P R IH1 N T S', 'M IH1 S P R IH0 N T S', 'V\\\\\\n']\n",
      "['MISUSE', 'M IH0 S Y UW1 S', 'M IH0 S Y UW1 Z', 'V\\\\\\n']\n",
      "['MISUSES', 'M IH0 S Y UW1 Z IH0 Z', 'M IH0 S Y UW1 S IH0 Z', 'V\\\\\\n']\n",
      "['MODERATE', 'M AA1 D ER0 EY2 T', 'M AA1 D ER0 AH0 T', 'V\\\\\\n']\n",
      "['MODERATES', 'M AA1 D ER0 EY2 T S', 'M AA1 D ER0 AH0 T S', 'V\\\\\\n']\n",
      "['MOUTH', 'M AW1 TH', 'M AW1 DH', 'V\\\\\\n']\n",
      "['MOUTHS', 'M AW1 DH Z', 'M AW1 TH S', 'V\\\\\\n']\n",
      "['OBJECT', 'AA1 B JH EH0 K T', 'AH0 B JH EH1 K T', 'V\\\\\\n']\n",
      "['OBJECTS', 'AH0 B JH EH1 K T S', 'AA1 B JH EH0 K T S', 'V\\\\\\n']\n",
      "['ORNAMENT', 'AO1 R N AH0 M EH0 N T', 'AO1 R N AH0 M AH0 N T', 'V\\\\\\n']\n",
      "['ORNAMENTS', 'AO1 R N AH0 M EH0 N T S', 'AO1 R N AH0 M AH0 N T S', 'V\\\\\\n']\n",
      "['OVERCHARGE', 'OW2 V ER0 CH AA1 R JH', 'OW1 V ER0 CH AA2 R JH', 'V\\\\\\n']\n",
      "['OVERCHARGES', 'OW2 V ER0 CH AA1 R JH IH0 Z', 'OW1 V ER0 CH AA2 R JH IH0 Z', 'V\\\\\\n']\n",
      "['OVERFLOW', 'OW2 V ER0 F L OW1', 'OW1 V ER0 F L OW2', 'V\\\\\\n']\n",
      "['OVERFLOWS', 'OW2 V ER0 F L OW1 Z', 'OW1 V ER0 F L OW2 Z', 'V\\\\\\n']\n",
      "['OVERHANG', 'OW2 V ER0 HH AE1 NG', 'OW1 V ER0 HH AE2 NG', 'V\\\\\\n']\n",
      "['OVERHANGS', 'OW2 V ER0 HH AE1 NG Z', 'OW1 V ER0 HH AE2 NG Z', 'V\\\\\\n']\n",
      "['OVERHAUL', 'OW2 V ER0 HH AO1 L', 'OW1 V ER0 HH AO2 L', 'V\\\\\\n']\n",
      "['OVERHAULS', 'OW2 V ER0 HH AO1 L Z', 'OW1 V ER0 HH AO2 L Z', 'V\\\\\\n']\n",
      "['OVERLAP', 'OW2 V ER0 L AE1 P', 'OW1 V ER0 L AE2 P', 'V\\\\\\n']\n",
      "['OVERLAPS', 'OW2 V ER0 L AE1 P S', 'OW1 V ER0 L AE2 P S', 'V\\\\\\n']\n",
      "['OVERLAY', 'OW2 V ER0 L EY1', 'OW1 V ER0 L EY2', 'V\\\\\\n']\n",
      "['OVERLAYS', 'OW2 V ER0 L EY1 Z', 'OW1 V ER0 L EY2 Z', 'V\\\\\\n']\n",
      "['OVERWORK', 'OW2 V ER0 W ER1 K', 'OW1 V ER0 W ER2 K', 'V\\\\\\n']\n",
      "['PERFECT', 'P ER0 F EH1 K T', 'P ER1 F IH2 K T', 'V\\\\\\n']\n",
      "['PERFUME', 'P ER0 F Y UW1 M', 'P ER1 F Y UW0 M', 'V\\\\\\n']\n",
      "['PERFUMES', 'P ER0 F Y UW1 M Z', 'P ER1 F Y UW0 M Z', 'V\\\\\\n']\n",
      "['PERMIT', 'P ER0 M IH1 T', 'P ER1 M IH2 T', 'V\\\\\\n']\n",
      "['PERMITS', 'P ER0 M IH1 T S', 'P ER1 M IH2 T S', 'V\\\\\\n']\n",
      "['PERVERT', 'P ER0 V ER1 T', 'P ER1 V ER0 T', 'V\\\\\\n']\n",
      "['PERVERTS', 'P ER0 V ER1 T S', 'P ER1 V ER0 T S', 'V\\\\\\n']\n",
      "['PONTIFICATE', 'P AA0 N T IH1 F AH0 K AH0 T', 'P AA0 N T IH1 F AH0 K EY2 T', 'V\\\\\\n']\n",
      "['PONTIFICATES', 'P AA0 N T IH1 F AH0 K EY2 T S', 'P AA0 N T IH1 F AH0 K AH0 T S', 'V\\\\\\n']\n",
      "['PRECIPITATE', 'P R IH0 S IH1 P IH0 T AH0 T', 'P R IH0 S IH1 P IH0 T EY2 T', 'V\\\\\\n']\n",
      "['PREDICATE', 'P R EH1 D IH0 K AH0 T', 'P R EH1 D AH0 K EY2 T', 'V\\\\\\n']\n",
      "['PREDICATES', 'P R EH1 D AH0 K EY2 T S', 'P R EH1 D IH0 K AH0 T S', 'V\\\\\\n']\n",
      "['PREFIX', 'P R IY2 F IH1 K S', 'P R IY1 F IH0 K S', 'V\\\\\\n']\n",
      "['PREFIXES', 'P R IY2 F IH1 K S IH0 JH', 'P R IY1 F IH0 K S IH0 JH', 'V\\\\\\n']\n",
      "['PRESAGE', 'P R EH2 S IH1 JH', 'P R EH1 S IH0 JH', 'V\\\\\\n']\n",
      "['PRESAGES', 'P R EH2 S IH1 JH IH0 JH', 'P R EH1 S IH0 JH IH0 JH', 'V\\\\\\n']\n",
      "['PRESENT', 'P R IY0 Z EH1 N T', 'P R EH1 Z AH0 N T', 'V\\\\\\n']\n",
      "['PRESENTS', 'P R IY0 Z EH1 N T S', 'P R EH1 Z AH0 N T S', 'V\\\\\\n']\n",
      "['PROCEEDS', 'P R AH0 S IY1 D Z', 'P R OW1 S IY0 D Z', 'V\\\\\\n']\n",
      "['PROCESS', 'P R AO2 S EH1 S', 'P R AA1 S EH2 S', 'V\\\\\\n']\n",
      "['PROCESSES', 'P R AA1 S EH0 S AH0 Z', 'P R AO2 S EH1 S AH0 Z', 'V\\\\\\n']\n",
      "['PROCESSING', 'P R AA0 S EH1 S IH0 NG', 'P R AA1 S EH0 S IH0 NG', 'V\\\\\\n']\n",
      "['PRODUCE', 'P R AH0 D UW1 S', 'P R OW1 D UW0 S', 'V\\\\\\n']\n",
      "['PROGRESS', 'P R AH0 G R EH1 S', 'P R AA1 G R EH2 S', 'V\\\\\\n']\n",
      "['PROGRESSES', 'P R OW0 G R EH1 S AH0 Z', 'P R AA1 G R EH2 S AH0 Z', 'V\\\\\\n']\n",
      "['PROJECT', 'P R AA0 JH EH1 K T', 'P R AA1 JH EH0 K T', 'V\\\\\\n']\n",
      "['PROJECTS', 'P R AA0 JH EH1 K T S', 'P R AA1 JH EH0 K T S', 'V\\\\\\n']\n",
      "['PROSPECT', 'P R AH2 S P EH1 K T', 'P R AA1 S P EH0 K T', 'V\\\\\\n']\n",
      "['PROSPECTS', 'P R AH2 S P EH1 K T S', 'P R AA1 S P EH0 K T S', 'V\\\\\\n']\n",
      "['PROSTRATE', 'P R AA0 S T R EY1 T', 'P R AA1 S T R EY0 T', 'V\\\\\\n']\n",
      "['PROTEST', 'P R AH0 T EH1 S T', 'P R OW1 T EH2 S T', 'V\\\\\\n']\n",
      "['PROTESTS', 'P R AH0 T EH1 S T S', 'P R OW1 T EH2 S T S', 'V\\\\\\n']\n",
      "['PURPORT', 'P ER0 P AO1 R T', 'P ER1 P AO2 R T', 'V\\\\\\n']\n",
      "['QUADRUPLE', 'K W AA1 D R UW0 P AH0 L', 'K W AA0 D R UW1 P AH0 L', 'V\\\\\\n']\n",
      "['QUADRUPLES', 'K W AA0 D R UW1 P AH0 L Z', 'K W AA1 D R UW0 P AH0 L Z', 'V\\\\\\n']\n",
      "['RAGGED', 'R AE1 G D', 'R AE1 G AH0 D', 'V\\\\\\n']\n",
      "['RAMPAGE', 'R AE2 M P EY1 JH', 'R AE1 M P EY2 JH', 'V\\\\\\n']\n",
      "['RAMPAGES', 'R AE2 M P EY1 JH IH0 Z', 'R AE1 M P EY2 JH IH0 Z', 'V\\\\\\n']\n",
      "['READ', 'R IY1 D', 'R EH1 D', 'VBD\\\\\\n']\n",
      "['REBEL', 'R EH1 B AH0 L', 'R IH0 B EH1 L', 'V\\\\\\n']\n",
      "['REBELS', 'R IH0 B EH1 L Z', 'R EH1 B AH0 L Z', 'V\\\\\\n']\n",
      "['REBOUND', 'R IY0 B AW1 N D', 'R IY1 B AW0 N D', 'V\\\\\\n']\n",
      "['REBOUNDS', 'R IY0 B AW1 N D Z', 'R IY1 B AW0 N D Z', 'V\\\\\\n']\n",
      "['RECALL', 'R IH0 K AO1 L', 'R IY1 K AO2 L', 'V\\\\\\n']\n",
      "['RECALLS', 'R IH0 K AO1 L Z', 'R IY1 K AO2 L Z', 'V\\\\\\n']\n",
      "['RECAP', 'R IH0 K AE1 P', 'R IY1 K AE2 P', 'V\\\\\\n']\n",
      "['RECAPPED', 'R IH0 K AE1 P T', 'R IY1 K AE2 P T', 'V\\\\\\n']\n",
      "['RECAPPING', 'R IH0 K AE1 P IH0 NG', 'R IY1 K AE2 P IH0 NG', 'V\\\\\\n']\n",
      "['RECAPS', 'R IH0 K AE1 P S', 'R IY1 K AE2 P S', 'V\\\\\\n']\n",
      "['RECOUNT', 'R IY2 K AW1 N T', ' R IH1 K AW0 N T', 'V\\\\\\n']\n",
      "['RECOUNTS', 'R IY2 K AW1 N T S', ' R IH1 K AW0 N T S', 'V\\\\\\n']\n",
      "['RECORD', 'R IH0 K AO1 R D', 'R EH1 K ER0 D', 'V\\\\\\n']\n",
      "['RECORDS', 'R IH0 K AO1 R D Z', 'R EH1 K ER0 D Z', 'V\\\\\\n']\n",
      "['REFILL', 'R IY0 F IH1 L', 'R IY1 F IH0 L', 'V\\\\\\n']\n",
      "['REFILLS', 'R IY0 F IH1 L Z', 'R IY1 F IH0 L Z', 'V\\\\\\n']\n",
      "['REFIT', 'R IY0 F IH1 T', 'R IY1 F IH0 T', 'V\\\\\\n']\n",
      "['REFITS', 'R IY0 F IH1 T S', 'R IY1 F IH0 T S', 'V\\\\\\n']\n",
      "['REFRESH', 'R IH0 F R EH1 SH', 'R IH1 F R EH0 SH', 'V\\\\\\n']\n",
      "['REFUND', 'R IH0 F AH1 N D', 'R IY1 F AH2 N D', 'V\\\\\\n']\n",
      "['REFUNDS', 'R IH0 F AH1 N D Z', 'R IY1 F AH2 N D Z', 'V\\\\\\n']\n",
      "['REFUSE', 'R IH0 F Y UW1 Z', 'R EH1 F Y UW2 Z', 'V\\\\\\n']\n",
      "['REGENERATE', 'R IY0 JH EH1 N ER0 EY2 T', 'R IY0 JH EH1 N ER0 AH0 T', 'V\\\\\\n']\n",
      "['REHASH', 'R IY0 HH AE1 SH', 'R IY1 HH AE0 SH', 'V\\\\\\n']\n",
      "['REHASHES', 'R IY0 HH AE1 SH IH0 Z', 'R IY1 HH AE0 SH IH0 Z', 'V\\\\\\n']\n",
      "['REINCARNATE', 'R IY2 IH0 N K AA1 R N EY2 T', 'R IY2 IH0 N K AA1 R N AH0 T', 'V\\\\\\n']\n",
      "['REJECT', 'R IH0 JH EH1 K T', 'R IY1 JH EH0 K T', 'V\\\\\\n']\n",
      "['REJECTS', 'R IH0 JH EH1 K T S', 'R IY1 JH EH0 K T S', 'V\\\\\\n']\n",
      "['RELAY', 'R IY2 L EY1', 'R IY1 L EY2', 'V\\\\\\n']\n",
      "['RELAYING', 'R IY2 L EY1 IH0 NG', 'R IY1 L EY2 IH0 NG', 'V\\\\\\n']\n",
      "['RELAYS', 'R IY2 L EY1 Z', 'R IY1 L EY2 Z', 'V\\\\\\n']\n",
      "['REMAKE', 'R IY2 M EY1 K', 'R IY1 M EY0 K', 'V\\\\\\n']\n",
      "['REMAKES', 'R IY2 M EY1 K S', 'R IY1 M EY0 K S', 'V\\\\\\n']\n",
      "['REPLAY', 'R IY0 P L EY1', 'R IY1 P L EY0', 'V\\\\\\n']\n",
      "['REPLAYS', 'R IY0 P L EY1 Z', 'R IY1 P L EY0 Z', 'V\\\\\\n']\n",
      "['REPRINT', 'R IY0 P R IH1 N T', 'R IY1 P R IH0 N T', 'V\\\\\\n']\n",
      "['REPRINTS', 'R IY0 P R IH1 N T S', 'R IY1 P R IH0 N T S', 'V\\\\\\n']\n",
      "['RERUN', 'R IY2 R AH1 N', 'R IY1 R AH0 N', 'V\\\\\\n']\n",
      "['RERUNS', 'R IY2 R AH1 N Z', 'R IY1 R AH0 N Z', 'V\\\\\\n']\n",
      "['RESUME', 'R IY0 Z UW1 M', 'R EH1 Z AH0 M EY2', 'V\\\\\\n']\n",
      "['RETAKE', 'R IY0 T EY1 K', 'R IY1 T EY0 K', 'V\\\\\\n']\n",
      "['RETAKES', 'R IY0 T EY1 K S', 'R IY1 T EY0 K S', 'V\\\\\\n']\n",
      "['RETHINK', 'R IY2 TH IH1 NG K', 'R IY1 TH IH0 NG K', 'V\\\\\\n']\n",
      "['RETHINKS', 'R IY2 TH IH1 NG K S', 'R IY1 TH IH0 NG K S', 'V\\\\\\n']\n",
      "['RETREAD', 'R IY2 T R EH1 D', 'R IY1 T R EH0 D', 'V\\\\\\n']\n",
      "['RETREADS', 'R IY2 T R EH1 D Z', 'R IY1 T R EH0 D Z', 'V\\\\\\n']\n",
      "['REWRITE', 'R IY0 R AY1 T', 'R IY1 R AY2 T', 'V\\\\\\n']\n",
      "['REWRITES', 'R IY0 R AY1 T S', 'R IY1 R AY2 T S', 'V\\\\\\n']\n",
      "['SEGMENT', 'S EH1 G M AH0 N T', 'S EH2 G M EH1 N T', 'V\\\\\\n']\n",
      "['SEGMENTS', 'S EH2 G M EH1 N T S', 'S EH1 G M AH0 N T S', 'V\\\\\\n']\n",
      "['SEPARATE', 'S EH1 P ER0 EY2 T', 'S EH1 P ER0 IH0 T', 'V\\\\\\n']\n",
      "['SEPARATES', 'S EH1 P ER0 EY2 T S', 'S EH1 P ER0 IH0 T S', 'V\\\\\\n']\n",
      "['SUBCONTRACT', 'S AH0 B K AA1 N T R AE2 K T', 'S AH2 B K AA0 N T R AE1 K T', 'V\\\\\\n']\n",
      "['SUBCONTRACTS', 'S AH2 B K AA0 N T R AE1 K T S', 'S AH0 B K AA1 N T R AE2 K T S', 'V\\\\\\n']\n",
      "['SUBJECT', 'S AH0 B JH EH1 K T', 'S AH1 B JH IH0 K T', 'V\\\\\\n']\n",
      "['SUBJECTS', 'S AH0 B JH EH1 K T S', 'S AH1 B JH IH0 K T S', 'V\\\\\\n']\n",
      "['SUBORDINATE', 'S AH0 B AO1 R D AH0 N EY2 T', 'S AH0 B AO1 R D AH0 N AH0 T', 'V\\\\\\n']\n",
      "['SUBORDINATES', 'S AH0 B AO1 R D AH0 N EY2 T S', 'S AH0 B AO1 R D AH0 N AH0 T S', 'V\\\\\\n']\n",
      "['SUPPLEMENT', 'S AH1 P L AH0 M EH0 N T', 'S AH1 P L AH0 M AH0 N T', 'V\\\\\\n']\n",
      "['SUPPLEMENTS', 'S AH1 P L AH0 M EH0 N T S', 'S AH1 P L AH0 M AH0 N T S', 'V\\\\\\n']\n",
      "['SURMISE', 'S ER0 M AY1 Z', 'S ER1 M AY0 Z', 'V\\\\\\n']\n",
      "['SURMISES', 'S ER0 M AY1 Z IH0 Z', 'S ER1 M AY0 Z IH0 Z', 'V\\\\\\n']\n",
      "['SURVEY', 'S ER0 V EY1', 'S ER1 V EY2', 'V\\\\\\n']\n",
      "['SURVEYS', 'S ER0 V EY1 Z', 'S ER1 V EY2 Z', 'V\\\\\\n']\n",
      "['SUSPECT', 'S AH0 S P EH1 K T', 'S AH1 S P EH2 K T', 'V\\\\\\n']\n",
      "['SUSPECTS', 'S AH0 S P EH1 K T S', 'S AH1 S P EH2 K T S', 'V\\\\\\n']\n",
      "['SYNDICATE', 'S IH1 N D AH0 K EY2 T', 'S IH1 N D IH0 K AH0 T', 'V\\\\\\n']\n",
      "['SYNDICATES', 'S IH1 N D IH0 K EY2 T S', 'S IH1 N D IH0 K AH0 T S', 'V\\\\\\n']\n",
      "['TORMENT', 'T AO1 R M EH2 N T', 'T AO0 R M EH1 N T', 'V\\\\\\n']\n",
      "['TRANSFER', 'T R AE0 N S F ER1', 'T R AE1 N S F ER0', 'V\\\\\\n']\n",
      "['TRANSFERS', 'T R AE0 N S F ER1 Z', 'T R AE1 N S F ER0 Z', 'V\\\\\\n']\n",
      "['TRANSPLANT', 'T R AE0 N S P L AE1 N T', 'T R AE1 N S P L AE0 N T', 'V\\\\\\n']\n",
      "['TRANSPLANTS', 'T R AE0 N S P L AE1 N T S', 'T R AE1 N S P L AE0 N T S', 'V\\\\\\n']\n",
      "['TRANSPORT', 'T R AE0 N S P AO1 R T', 'T R AE1 N S P AO0 R T', 'V\\\\\\n']\n",
      "['TRANSPORTS', 'T R AE0 N S P AO1 R T S', 'T R AE1 N S P AO0 R T S', 'V\\\\\\n']\n",
      "['TRIPLICATE', 'T R IH1 P L IH0 K EY2 T', 'T R IH1 P L IH0 K AH0 T', 'V\\\\\\n']\n",
      "['TRIPLICATES', 'T R IH1 P L IH0 K EY2 T S', 'T R IH1 P L IH0 K AH0 T S', 'V\\\\\\n']\n",
      "['UNDERCUT', 'AH2 N D ER0 K AH1 T', 'AH1 N D ER0 K AH2 T', 'V\\\\\\n']\n",
      "['UNDERESTIMATE', 'AH1 N D ER0 EH1 S T AH0 M EY2 T', 'AH1 N D ER0 EH1 S T AH0 M AH0 T', 'V\\\\\\n']\n",
      "['UNDERESTIMATES', 'AH1 N D ER0 EH1 S T AH0 M EY2 T S', 'AH1 N D ER0 EH1 S T AH0 M AH0 T S', 'V\\\\\\n']\n",
      "['UNDERLINE', 'AH2 N D ER0 L AY1 N', 'AH1 N D ER0 L AY2 N', 'V\\\\\\n']\n",
      "['UNDERLINES', 'AH2 N D ER0 L AY1 N Z', 'AH1 N D ER0 L AY2 N Z', 'V\\\\\\n']\n",
      "['UNDERTAKING', 'AH2 N D ER0 T EY1 K IH0 NG', 'AH1 N D ER0 T EY2 K IH0 NG', 'V\\\\\\n']\n",
      "['UNDERTAKINGS', 'AH2 N D ER0 T EY1 K IH0 NG Z', 'AH1 N D ER0 T EY2 K IH0 NG Z', 'V\\\\\\n']\n",
      "['UNUSED', 'AH0 N Y UW1 Z D', 'AH0 N Y UW1 S T', 'V\\\\\\n']\n",
      "['UPGRADE', 'AH0 P G R EY1 D', 'AH1 P G R EY0 D', 'V\\\\\\n']\n",
      "['UPGRADES', 'AH0 P G R EY1 D Z', 'AH1 P G R EY0 D Z', 'V\\\\\\n']\n",
      "['UPLIFT', 'AH2 P L IH1 F T', 'AH1 P L IH0 F T', 'V\\\\\\n']\n",
      "['UPSET', 'AH0 P S EH1 T', 'AH1 P S EH2 T', 'V\\\\\\n']\n",
      "['UPSETS', 'AH0 P S EH1 T S', 'AH1 P S EH2 T S', 'V\\\\\\n']\n",
      "['USE', 'Y UW1 Z', 'Y UW1 S', 'V\\\\\\n']\n",
      "['USED', 'Y UW1 Z D', 'Y UW1 S T', 'VBN\\\\\\n']\n",
      "['USES', 'Y UW1 Z IH0 Z', 'Y UW1 S IH0 Z', 'V}']\n",
      "371\n"
     ]
    }
   ],
   "source": [
    "#G2P\n",
    "\n",
    "f = open(\"homograph.txt\")\n",
    "homograph = dict()\n",
    "for line in f.readlines()[10:]:\n",
    "  if line.startswith(\"#\"):continue\n",
    "  l = line.split(\"|\")\n",
    "  print(l)\n",
    "  homograph[l[0].lower()] = (l[1].split(), l[2].split(), l[3].replace(\"\\\\\\n\", \"\"))\n",
    "print(len(homograph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ca7f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import cmudict\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "word_tokenize = TweetTokenizer().tokenize\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "from builtins import str as unicode\n",
    "\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger.zip')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "try:\n",
    "    nltk.data.find('corpora/cmudict.zip')\n",
    "except LookupError:\n",
    "    nltk.download('cmudict')\n",
    "\n",
    "\n",
    "\n",
    "def construct_homograph_dictionary():\n",
    "    homograph2features = dict()\n",
    "    f = open(\"homograph.txt\")\n",
    "    for line in f.readlines()[10:]:\n",
    "        #print(line)\n",
    "        if line.startswith(\"#\"): continue # comment\n",
    "        headword, pron1, pron2, pos1 = line.split(\"|\")[0], line.split(\"|\")[1],line.split(\"|\")[2],line.split(\"|\")[3].replace(\"\\\\\\n\", \"\")\n",
    "        homograph2features[headword.lower()] = (pron1.split(), pron2.split(), pos1)\n",
    "    return homograph2features\n",
    "\n",
    "\n",
    "class G2p(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.graphemes = [\"<pad>\", \"<unk>\", \"</s>\"] + list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "        self.phonemes = [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"] + ['AA0', 'AA1', 'AA2', 'AE0', 'AE1', 'AE2', 'AH0', 'AH1', 'AH2', 'AO0',\n",
    "                                                             'AO1', 'AO2', 'AW0', 'AW1', 'AW2', 'AY0', 'AY1', 'AY2', 'B', 'CH', 'D', 'DH',\n",
    "                                                             'EH0', 'EH1', 'EH2', 'ER0', 'ER1', 'ER2', 'EY0', 'EY1',\n",
    "                                                             'EY2', 'F', 'G', 'HH',\n",
    "                                                             'IH0', 'IH1', 'IH2', 'IY0', 'IY1', 'IY2', 'JH', 'K', 'L',\n",
    "                                                             'M', 'N', 'NG', 'OW0', 'OW1',\n",
    "                                                             'OW2', 'OY0', 'OY1', 'OY2', 'P', 'R', 'S', 'SH', 'T', 'TH',\n",
    "                                                             'UH0', 'UH1', 'UH2', 'UW',\n",
    "                                                             'UW0', 'UW1', 'UW2', 'V', 'W', 'Y', 'Z', 'ZH']\n",
    "        self.g2idx = {g: idx for idx, g in enumerate(self.graphemes)}\n",
    "        self.idx2g = {idx: g for idx, g in enumerate(self.graphemes)}\n",
    "\n",
    "        self.p2idx = {p: idx for idx, p in enumerate(self.phonemes)}\n",
    "        self.idx2p = {idx: p for idx, p in enumerate(self.phonemes)}\n",
    "\n",
    "        self.cmu = cmudict.dict()\n",
    "        self.load_variables()\n",
    "        self.homograph2features = construct_homograph_dictionary()\n",
    "\n",
    "    def load_variables(self):\n",
    "        self.variables = np.load(\"checkpoint20.npz\")\n",
    "        self.enc_emb = self.variables[\"enc_emb\"]  # (29, 64). (len(graphemes), emb)\n",
    "        self.enc_w_ih = self.variables[\"enc_w_ih\"]  # (3*128, 64)\n",
    "        self.enc_w_hh = self.variables[\"enc_w_hh\"]  # (3*128, 128)\n",
    "        self.enc_b_ih = self.variables[\"enc_b_ih\"]  # (3*128,)\n",
    "        self.enc_b_hh = self.variables[\"enc_b_hh\"]  # (3*128,)\n",
    "\n",
    "        self.dec_emb = self.variables[\"dec_emb\"]  # (74, 64). (len(phonemes), emb)\n",
    "        self.dec_w_ih = self.variables[\"dec_w_ih\"]  # (3*128, 64)\n",
    "        self.dec_w_hh = self.variables[\"dec_w_hh\"]  # (3*128, 128)\n",
    "        self.dec_b_ih = self.variables[\"dec_b_ih\"]  # (3*128,)\n",
    "        self.dec_b_hh = self.variables[\"dec_b_hh\"]  # (3*128,)\n",
    "        self.fc_w = self.variables[\"fc_w\"]  # (74, 128)\n",
    "        self.fc_b = self.variables[\"fc_b\"]  # (74,)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x)) # mathematical logistic function // for the activation of the neural network\n",
    "\n",
    "    def grucell(self, x, h, w_ih, w_hh, b_ih, b_hh):\n",
    "        rzn_ih = np.matmul(x, w_ih.T) + b_ih # matrix_product of two array\n",
    "        rzn_hh = np.matmul(h, w_hh.T) + b_hh\n",
    "\n",
    "        rz_ih, n_ih = rzn_ih[:, :rzn_ih.shape[-1] * 2 // 3], rzn_ih[:, rzn_ih.shape[-1] * 2 // 3:]\n",
    "        rz_hh, n_hh = rzn_hh[:, :rzn_hh.shape[-1] * 2 // 3], rzn_hh[:, rzn_hh.shape[-1] * 2 // 3:]\n",
    "\n",
    "        rz = self.sigmoid(rz_ih + rz_hh)\n",
    "        r, z = np.split(rz, 2, -1)\n",
    "\n",
    "        n = np.tanh(n_ih + r * n_hh)\n",
    "        h = (1 - z) * n + z * h\n",
    "\n",
    "        return h\n",
    "\n",
    "    def gru(self, x, steps, w_ih, w_hh, b_ih, b_hh, h0=None):\n",
    "        if h0 is None:\n",
    "            h0 = np.zeros((x.shape[0], w_hh.shape[1]), np.float32)\n",
    "        h = h0  # initial hidden state\n",
    "        outputs = np.zeros((x.shape[0], steps, w_hh.shape[1]), np.float32)\n",
    "        for t in range(steps):\n",
    "            h = self.grucell(x[:, t, :], h, w_ih, w_hh, b_ih, b_hh)  # (b, h)\n",
    "            outputs[:, t, ::] = h\n",
    "        return outputs\n",
    "\n",
    "    def encode(self, word):\n",
    "        chars = list(word) + [\"</s>\"]\n",
    "        x = [self.g2idx.get(char, self.g2idx[\"<unk>\"]) for char in chars]\n",
    "        x = np.take(self.enc_emb, np.expand_dims(x, 0), axis=0)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict(self, word):\n",
    "        # encoder\n",
    "        enc = self.encode(word)\n",
    "        enc = self.gru(enc, len(word) + 1, self.enc_w_ih, self.enc_w_hh,\n",
    "                       self.enc_b_ih, self.enc_b_hh, h0=np.zeros((1, self.enc_w_hh.shape[-1]), np.float32))\n",
    "        last_hidden = enc[:, -1, :]\n",
    "\n",
    "        # decoder\n",
    "        dec = np.take(self.dec_emb, [2], axis=0)  # 2: <s>\n",
    "        h = last_hidden\n",
    "\n",
    "        preds = []\n",
    "        for i in range(20):\n",
    "            h = self.grucell(dec, h, self.dec_w_ih, self.dec_w_hh, self.dec_b_ih, self.dec_b_hh)  # (b, h)\n",
    "            logits = np.matmul(h, self.fc_w.T) + self.fc_b\n",
    "            pred = logits.argmax()\n",
    "            if pred == 3: break  # 3: </s>\n",
    "            preds.append(pred)\n",
    "            dec = np.take(self.dec_emb, [pred], axis=0)\n",
    "\n",
    "        preds = [self.idx2p.get(idx, \"<unk>\") for idx in preds]\n",
    "        return preds\n",
    "\n",
    "    def __call__(self, text):\n",
    "        # preprocessing\n",
    "        text = unicode(text)\n",
    "        text = normalize_numbers(text)\n",
    "        text = ''.join(char for char in unicodedata.normalize('NFD', text) #Convert the text into its decomposed form\n",
    "                       if unicodedata.category(char) != 'Mn')  # Strip accents\n",
    "        text = text.lower()\n",
    "        text = re.sub(\"[^ a-z'.,?!\\-]\", \"\", text)\n",
    "        text = text.replace(\"i.e.\", \"that is\")\n",
    "        text = text.replace(\"e.g.\", \"for example\")\n",
    "\n",
    "        # tokenization\n",
    "        words = word_tokenize(text)\n",
    "        tokens = pos_tag(words)  # tuples of (word, tag)\n",
    "\n",
    "        # steps\n",
    "        prons = []\n",
    "        for word, pos in tokens:\n",
    "            if re.search(\"[a-z]\", word) is None:\n",
    "                pron = [word]\n",
    "\n",
    "            elif word in self.homograph2features:  # Check homograph\n",
    "                pron1, pron2, pos1 = self.homograph2features[word]\n",
    "                if pos.startswith(pos1):\n",
    "                    pron = pron1\n",
    "                else:\n",
    "                    pron = pron2\n",
    "            elif word in self.cmu:  # lookup CMU dict\n",
    "                pron = self.cmu[word][0]\n",
    "            else: # predict for oov\n",
    "                pron = self.predict(word)\n",
    "\n",
    "            prons.extend(pron)\n",
    "            prons.extend([\" \"])\n",
    "\n",
    "        return prons[:-1]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    texts = [\"I have $250 in my pocket.\", # number -> spell-out\n",
    "             \"popular pets, e.g. cats and dogs\", # e.g. -> for example\n",
    "             \"I refuse to collect the refuse around here.\", # homograph\n",
    "             \"I'm an activationist.\"] # newly coined word\n",
    "    g2p = G2p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baf3f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "G2P_names = []\n",
    "G2P_names_test = []\n",
    "\n",
    "for i in range(X_test.size):\n",
    "    name = ''.join(g2p(X_test.tolist()[i]))\n",
    "    G2P_names_test.append(name)\n",
    "\n",
    "for i in range(X_train.size):\n",
    "    name = ''.join(g2p(X_train.tolist()[i]))\n",
    "    G2P_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f50468e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "# from aion.embeddings.doc2vec import Doc2VecEmbeddings\n",
    "\n",
    "model = gensim.models.Word2Vec(G2P_names, min_count = 1,vector_size = \n",
    "                                50, window = 4, sg=0)\n",
    "\n",
    "# def tagged_document(list_of_list_of_words):\n",
    "#    for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "#       yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "        \n",
    "# data_for_training = list(tagged_document(G2P_names))\n",
    "# print(data_for_training [:2])\n",
    "\n",
    "# fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "\n",
    "# model = gensim.models.doc2vec.Doc2Vec(vector_size=50,window=2,alpha = 0.025,epochs=50,min_count=1)\n",
    "model.build_vocab(G2P_names)\n",
    "model.train(G2P_names, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "# model.save(fname)\n",
    "\n",
    "word_vectors = model.wv\n",
    "word_vectors.save(\"word2vec.wordvectors\")\n",
    "wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n",
    "\n",
    "\n",
    "# def train_model(epochs, vector_size, window):\n",
    "#     documents = [TaggedDocument(list(doc), [i]) for i, doc in enumerate(G2P_names)]\n",
    "#     model = Doc2Vec(documents, epochs=epochs, vector_size=vector_size, window=window, workers=1)\n",
    "#     return model\n",
    "\n",
    "# doc2vec_embs = Doc2VecEmbeddings()\n",
    "# x_train_tokens = doc2vec_embs.build_vocab(documents=G2P_names)\n",
    "# doc2vec_embs.train(x_train_tokens)\n",
    "\n",
    "# x_train_t = doc2vec_embs.encode(documents=G2P_names)\n",
    "# x_test_t = doc2vec_embs.encode(documents=G2P_names_test)\n",
    "\n",
    "# name_vector = []\n",
    "# name_vector_test = []\n",
    "\n",
    "# for i in range(len(G2P_names)):\n",
    "#     vector = model2.docvecs[i]\n",
    "#     name_vector.append(vector)\n",
    "    \n",
    "# for i in range(len(G2P_names_test)):\n",
    "#     vector = model2.infer_vector([G2P_names_test[i]])\n",
    "#     name_vector_test.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e7c52f1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key '60' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [151]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model = Doc2Vec.load(fname)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m x_train \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m      5\u001b[0m     vector \u001b[38;5;241m=\u001b[39m wv[i]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py:395\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py:438\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m \n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py:412\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key '60' not present\""
     ]
    }
   ],
   "source": [
    "# model = Doc2Vec.load(fname)\n",
    "x_train = []\n",
    "print(wv[60])\n",
    "for i in range(X_train.shape[0]):\n",
    "    vector = wv[i]\n",
    "    x_train.append(vector)\n",
    "# print(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e32f6f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=31)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#using KNN to predict\n",
    "neigh = KNeighborsClassifier(n_neighbors=31)\n",
    "\n",
    "# pipeline = Pipeline([('knn', neigh)])\n",
    "neigh.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8656ff33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "[28 28 28 ... 83 83 28]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.014453477868112014"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# predict gender of test names\n",
    "x_test_t = []\n",
    "# print(model.infer_vector([G2P_names_test[0]]))\n",
    "\n",
    "for i in range(len(G2P_names_test)):\n",
    "    vector = model.infer_vector([G2P_names_test[i]])\n",
    "    x_test_t.append(vector)\n",
    "\n",
    "y_pred = neigh.predict(x_test_t)\n",
    "n = \"\".join(g2p('Xiao'))\n",
    "name_vector2 = model.infer_vector([n])\n",
    "print(neigh.predict([name_vector2])[0])\n",
    "\n",
    "# y_pred = pipeline.predict([model.infer_vector([G2P_names_test[1]])])\n",
    "print(y_pred)\n",
    "\n",
    "# compute accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42809c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
