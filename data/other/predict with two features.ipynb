{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96cc0bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177042, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# read CSV into a pandas dataframe\n",
    "# download it here:\n",
    "# https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YPRQH8\n",
    "df = pd.read_csv('wgnd_noctry.csv',low_memory=False)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d7349d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90283</th>\n",
       "      <td>KURRI</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40570</th>\n",
       "      <td>DIDERICH</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65822</th>\n",
       "      <td>ISHEA</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79701</th>\n",
       "      <td>KAHLAYA</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144001</th>\n",
       "      <td>SHENITRA</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65110</th>\n",
       "      <td>INICE</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159727</th>\n",
       "      <td>TRAMPAS</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92884</th>\n",
       "      <td>LARONE</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138234</th>\n",
       "      <td>SASHIERE</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45088</th>\n",
       "      <td>ELADIO</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            name gender\n",
       "90283      KURRI      M\n",
       "40570   DIDERICH      M\n",
       "65822      ISHEA      F\n",
       "79701    KAHLAYA      F\n",
       "144001  SHENITRA      F\n",
       "...          ...    ...\n",
       "65110      INICE      F\n",
       "159727   TRAMPAS      M\n",
       "92884     LARONE      M\n",
       "138234  SASHIERE      F\n",
       "45088     ELADIO      M\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8fbb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude rows where gender is undefined\n",
    "df = df[df.gender.isin(['M', 'F'])]\n",
    "# define input\n",
    "X = df.name\n",
    "# define labels (female = 0, male = 1)\n",
    "y = df.gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b71dbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((149594,), (149594,), (16622,), (16622,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# set aside 10% of the dataset for test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.1)\n",
    "# inspect resulting shapes\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e684e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUMPY\n",
    "\n",
    "from __future__ import print_function\n",
    "import inflect\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "_inflect = inflect.engine()\n",
    "_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')\n",
    "_decimal_number_re = re.compile(r'([0-9]+\\.[0-9]+)')\n",
    "_pounds_re = re.compile(r'£([0-9\\,]*[0-9]+)')\n",
    "_dollars_re = re.compile(r'\\$([0-9\\.\\,]*[0-9]+)')\n",
    "_ordinal_re = re.compile(r'[0-9]+(st|nd|rd|th)')\n",
    "_number_re = re.compile(r'[0-9]+')\n",
    "\n",
    "\n",
    "def _remove_commas(m):\n",
    "    return m.group(1).replace(',', '')\n",
    "\n",
    "\n",
    "def _expand_decimal_point(m):\n",
    "    return m.group(1).replace('.', ' point ')\n",
    "\n",
    "\n",
    "def _expand_dollars(m):\n",
    "    match = m.group(1)\n",
    "    parts = match.split('.')\n",
    "    if len(parts) > 2:\n",
    "        return match + ' dollars'    # Unexpected format\n",
    "    dollars = int(parts[0]) if parts[0] else 0\n",
    "    cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n",
    "    if dollars and cents:\n",
    "        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n",
    "        cent_unit = 'cent' if cents == 1 else 'cents'\n",
    "        return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n",
    "    elif dollars:\n",
    "        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n",
    "        return '%s %s' % (dollars, dollar_unit)\n",
    "    elif cents:\n",
    "        cent_unit = 'cent' if cents == 1 else 'cents'\n",
    "        return '%s %s' % (cents, cent_unit)\n",
    "    else:\n",
    "        return 'zero dollars'\n",
    "\n",
    "\n",
    "def _expand_ordinal(m):\n",
    "    return _inflect.number_to_words(m.group(0))\n",
    "\n",
    "\n",
    "def _expand_number(m):\n",
    "    num = int(m.group(0))\n",
    "    if num > 1000 and num < 3000:\n",
    "        if num == 2000:\n",
    "            return 'two thousand'\n",
    "        elif num > 2000 and num < 2010:\n",
    "            return 'two thousand ' + _inflect.number_to_words(num % 100)\n",
    "        elif num % 100 == 0:\n",
    "            return _inflect.number_to_words(num // 100) + ' hundred'\n",
    "        else:\n",
    "            return _inflect.number_to_words(num, andword='', zero='oh', group=2).replace(', ', ' ')\n",
    "    else:\n",
    "        return _inflect.number_to_words(num, andword='')\n",
    "\n",
    "\n",
    "def normalize_numbers(text):\n",
    "    text = re.sub(_comma_number_re, _remove_commas, text)\n",
    "    text = re.sub(_pounds_re, r'\\1 pounds', text)\n",
    "    text = re.sub(_dollars_re, _expand_dollars, text)\n",
    "    text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n",
    "    text = re.sub(_ordinal_re, _expand_ordinal, text)\n",
    "    text = re.sub(_number_re, _expand_number, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd02e87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABSENT', 'AH1 B S AE1 N T', 'AE1 B S AH0 N T', 'V\\\\\\n']\n",
      "['ABSTRACT', 'AE0 B S T R AE1 K T', 'AE1 B S T R AE2 K T', 'V\\\\\\n']\n",
      "['ABSTRACTS', 'AE0 B S T R AE1 K T S', 'AE1 B S T R AE0 K T S', 'V\\\\\\n']\n",
      "['ABUSE', 'AH0 B Y UW1 Z', 'AH0 B Y UW1 S', 'V\\\\\\n']\n",
      "['ABUSES', 'AH0 B Y UW1 Z IH0 Z', 'AH0 B Y UW1 S IH0 Z', 'V\\\\\\n']\n",
      "['ACCENT', 'AH0 K S EH1 N T', 'AE1 K S EH2 N T', 'V\\\\\\n']\n",
      "['ACCENTS', 'AE1 K S EH0 N T S', 'AE1 K S EH0 N T S', 'V\\\\\\n']\n",
      "['ADDICT', 'AH0 D IH1 K T', 'AE1 D IH2 K T', 'V\\\\\\n']\n",
      "['ADDICTS', 'AH0 D IH1 K T S', 'AE1 D IH2 K T S', 'V\\\\\\n']\n",
      "['ADVOCATE', 'AE1 D V AH0 K EY2 T', 'AE1 D V AH0 K AH0 T', 'V\\\\\\n']\n",
      "['ADVOCATES', 'AE1 D V AH0 K EY2 T S', 'AE1 D V AH0 K AH0 T S', 'V\\\\\\n']\n",
      "['AFFECT', 'AH0 F EH1 K T', 'AE1 F EH0 K T', 'V\\\\\\n']\n",
      "['AFFECTS', 'AH0 F EH1 K T S', 'AE1 F EH0 K T S', 'V\\\\\\n']\n",
      "['AFFIX', 'AH0 F IH1 K S', 'AE1 F IH0 K S', 'V\\\\\\n']\n",
      "['AFFIXES', 'AH0 F IH1 K S IH0 Z', 'AE1 F IH0 K S IH0 Z', 'V\\\\\\n']\n",
      "['AGGLOMERATE', 'AH0 G L AA1 M ER0 EY2 T', 'AH0 G L AA1 M ER0 AH0 T', 'V\\\\\\n']\n",
      "['AGGREGATE', 'AE1 G R AH0 G EY0 T', 'AE1 G R AH0 G AH0 T', 'V\\\\\\n']\n",
      "['AGGREGATES', 'AE1 G R AH0 G EY2 T S', 'AE1 G R AH0 G IH0 T S', 'V\\\\\\n']\n",
      "['ALLIES', 'AH0 L AY1 Z', 'AE1 L AY0 Z', 'V\\\\\\n']\n",
      "['ALLOY', 'AH0 L OY1', 'AE1 L OY2', 'V\\\\\\n']\n",
      "['ALLOYS', 'AH0 L OY1 Z', 'AE1 L OY2 Z', 'V\\\\\\n']\n",
      "['ALLY', 'AH0 L AY1', 'AE1 L AY0', 'V\\\\\\n']\n",
      "['ALTERNATE', 'AO1 L T ER0 N EY2 T', 'AO0 L T ER1 N AH0 T', 'V\\\\\\n']\n",
      "['ANALYSES', 'AH0 N AE1 L IH0 S IY2 Z', 'AE1 N AH0 L AY0 Z IH2 Z', 'V\\\\\\n']\n",
      "['ANIMATE', 'AE1 N AH0 M EY2 T', 'AE1 N AH0 M AH0 T', 'V\\\\\\n']\n",
      "['ANNEX', 'AH0 N EH1 K S', 'AE1 N EH2 K S', 'V\\\\\\n']\n",
      "['ANNEXES', 'AH0 N EH1 K S IH0 Z', 'AE1 N EH2 K S IH0 Z', 'V\\\\\\n']\n",
      "['APPROPRIATE', 'AH0 P R OW1 P R IY0 EY2 T', 'AH0 P R OW1 P R IY0 AH0 T', 'V\\\\\\n']\n",
      "['APPROXIMATE', 'AH0 P R AA1 K S AH0 M EY2 T', 'AH0 P R AA1 K S AH0 M AH0 T', 'V\\\\\\n']\n",
      "['ARTICULATE', 'AA0 R T IH1 K Y AH0 L AH0 T', 'AA0 R T IH1 K Y AH0 L EY2 T', 'V\\\\\\n']\n",
      "['ASPIRATE', 'AE1 S P ER0 EY2 T', 'AE1 S P ER0 AH0 T', 'V\\\\\\n']\n",
      "['ASPIRATES', 'AE1 S P ER0 EY2 T S', 'AE1 S P ER0 AH0 T S', 'V\\\\\\n']\n",
      "['ASSOCIATE', 'AH0 S OW1 S IY0 EY2 T', 'AH0 S OW1 S IY0 AH0 T', 'V\\\\\\n']\n",
      "['ASSOCIATES', 'AH0 S OW1 S IY0 EY2 T S', 'AH0 S OW1 S IY0 AH0 T S', 'V\\\\\\n']\n",
      "['ATTRIBUTE', 'AH0 T R IH1 B Y UW2 T', 'AE1 T R IH0 B Y UW0 T', 'V\\\\\\n']\n",
      "['ATTRIBUTES', 'AH0 T R IH1 B Y UW2 T S', 'AE1 T R IH0 B Y UW0 T S', 'V\\\\\\n']\n",
      "['BATHS', 'B AE1 TH S', 'B AE1 DH Z', 'V\\\\\\n']\n",
      "['BLESSED', 'B L EH1 S IH0 D', 'B L EH1 S T', 'V\\\\\\n']\n",
      "['CERTIFICATE', 'S ER0 T IH1 F IH0 K AH0 T', 'S ER0 T IH1 F IH0 K EY2 T', 'V\\\\\\n']\n",
      "['CERTIFICATES', 'S ER0 T IH1 F IH0 K EY2 T S', 'S ER0 T IH1 F IH0 K AH0 T S', 'V\\\\\\n']\n",
      "['CLOSE', 'K L OW1 Z', 'K L OW1 S', 'V\\\\\\n']\n",
      "['CLOSER', 'K L OW1 Z ER0', 'K L OW1 S ER0', 'N\\\\\\n']\n",
      "['CLOSES', 'K L OW1 Z IH0 Z', 'K L OW1 S IH0 Z', 'V\\\\\\n']\n",
      "['COLLECT', 'K AH0 L EH1 K T', 'K AA1 L EH0 K T', 'V\\\\\\n']\n",
      "['COLLECTS', 'K AH0 L EH1 K T S', 'K AA1 L EH0 K T S', 'V\\\\\\n']\n",
      "['COMBAT', 'K AH0 M B AE1 T', 'K AA1 M B AE0 T', 'V\\\\\\n']\n",
      "['COMBATS', 'K AH0 M B AE1 T S', 'K AH1 M B AE0 T S', 'V\\\\\\n']\n",
      "['COMBINE', 'K AH0 M B AY1 N', 'K AA1 M B AY0 N', 'V\\\\\\n']\n",
      "['COMMUNE', 'K AH0 M Y UW1 N', 'K AA1 M Y UW0 N', 'V\\\\\\n']\n",
      "['COMMUNES', 'K AH0 M Y UW1 N Z', 'K AA1 M Y UW0 N Z', 'V\\\\\\n']\n",
      "['COMPACT', 'K AH0 M P AE1 K T', 'K AA1 M P AE0 K T', 'V\\\\\\n']\n",
      "['COMPACTS', 'K AH0 M P AE1 K T S', 'K AA1 M P AE0 K T S', 'V\\\\\\n']\n",
      "['COMPLEX', 'K AH0 M P L EH1 K S', ' K AA1 M P L EH0 K S', 'ADJ\\\\\\n']\n",
      "['COMPLIMENT', 'K AA1 M P L AH0 M EH0 N T', 'K AA1 M P L AH0 M AH0 N T', 'V\\\\\\n']\n",
      "['COMPLIMENTS', 'K AA1 M P L AH0 M EH0 N T S', 'K AA1 M P L AH0 M AH0 N T S', 'V\\\\\\n']\n",
      "['COMPOUND', 'K AH0 M P AW1 N D', 'K AA1 M P AW0 N D', 'V\\\\\\n']\n",
      "['COMPOUNDS', 'K AH0 M P AW1 N D Z', 'K AA1 M P AW0 N D Z', 'V\\\\\\n']\n",
      "['COMPRESS', 'K AH0 M P R EH1 S', 'K AA1 M P R EH0 S', 'V\\\\\\n']\n",
      "['COMPRESSES', 'K AH0 M P R EH1 S IH0 Z', 'K AA1 M P R EH0 S AH0 Z', 'V\\\\\\n']\n",
      "['CONCERT', 'K AH0 N S ER1 T', 'K AA1 N S ER0 T', 'V\\\\\\n']\n",
      "['CONCERTS', 'K AH0 N S ER1 T S', 'K AA1 N S ER0 T S', 'V\\\\\\n']\n",
      "['CONDUCT', 'K AA0 N D AH1 K T', 'K AA1 N D AH0 K T', 'V\\\\\\n']\n",
      "['CONFEDERATE', 'K AH0 N F EH1 D ER0 EY2 T', 'K AH0 N F EH1 D ER0 AH0 T', 'V\\\\\\n']\n",
      "['CONFEDERATES', 'K AH0 N F EH1 D ER0 EY2 T S', 'K AH0 N F EH1 D ER0 AH0 T S', 'V\\\\\\n']\n",
      "['CONFINES', 'K AH0 N F AY1 N Z', 'K AA1 N F AY2 N Z', 'V\\\\\\n']\n",
      "['CONFLICT', 'K AH0 N F L IH1 K T', 'K AA1 N F L IH0 K T', 'V\\\\\\n']\n",
      "['CONFLICTS', 'K AH0 N F L IH1 K T S', 'K AA1 N F L IH0 K T S', 'V\\\\\\n']\n",
      "['CONGLOMERATE', 'K AH0 N G L AA1 M ER0 EY2 T', 'K AH0 N G L AA1 M ER0 AH0 T', 'V\\\\\\n']\n",
      "['CONGLOMERATES', 'K AH0 N G L AA1 M ER0 EY2 T S', 'K AH0 N G L AA1 M ER0 AH0 T S', 'V\\\\\\n']\n",
      "['CONSCRIPT', 'K AH0 N S K R IH1 P T', 'K AA1 N S K R IH0 P T', 'V\\\\\\n']\n",
      "['CONSCRIPTS', 'K AH0 N S K R IH1 P T S', 'K AA1 N S K R IH0 P T S', 'V\\\\\\n']\n",
      "['CONSOLE', 'K AH0 N S OW1 L', 'K AA1 N S OW0 L', 'V\\\\\\n']\n",
      "['CONSOLES', 'K AH0 N S OW1 L Z', 'K AA1 N S OW0 L Z', 'V\\\\\\n']\n",
      "['CONSORT', 'K AH0 N S AO1 R T', 'K AA1 N S AO0 R T', 'V\\\\\\n']\n",
      "['CONSTRUCT', 'K AH0 N S T R AH1 K T', 'K AA1 N S T R AH0 K T', 'V\\\\\\n']\n",
      "['CONSTRUCTS', 'K AH0 N S T R AH1 K T S', 'K AA1 N S T R AH0 K T S', 'V\\\\\\n']\n",
      "['CONSUMMATE', 'K AA1 N S AH0 M EY2 T', 'K AA0 N S AH1 M AH0 T', 'V\\\\\\n']\n",
      "['CONTENT', 'K AA1 N T EH0 N T', 'K AH0 N T EH1 N T', 'N\\\\\\n']\n",
      "['CONTENTS', 'K AH0 N T EH1 N T S', 'K AA1 N T EH0 N T S', 'V\\\\\\n']\n",
      "['CONTEST', 'K AH0 N T EH1 S T', 'K AA1 N T EH0 S T', 'V\\\\\\n']\n",
      "['CONTESTS', 'K AH0 N T EH1 S T S', 'K AA1 N T EH0 S T S', 'V\\\\\\n']\n",
      "['CONTRACT', 'K AH0 N T R AE1 K T', 'K AA1 N T R AE2 K T', 'V\\\\\\n']\n",
      "['CONTRACTS', 'K AH0 N T R AE1 K T S', 'K AA1 N T R AE2 K T S', 'V\\\\\\n']\n",
      "['CONTRAST', 'K AH0 N T R AE1 S T', 'K AA1 N T R AE0 S T', 'V\\\\\\n']\n",
      "['CONTRASTS', 'K AH0 N T R AE1 S T S', 'K AA1 N T R AE0 S T S', 'V\\\\\\n']\n",
      "['CONVERSE', 'K AH0 N V ER1 S', 'K AA1 N V ER0 S', 'V\\\\\\n']\n",
      "['CONVERT', 'K AH0 N V ER1 T', 'K AA1 N V ER0 T', 'V\\\\\\n']\n",
      "['CONVERTS', 'K AH0 N V ER1 T S', 'K AA1 N V ER0 T S', 'V\\\\\\n']\n",
      "['CONVICT', 'K AH0 N V IH1 K T', 'K AA1 N V IH0 K T', 'V\\\\\\n']\n",
      "['CONVICTS', 'K AH0 N V IH1 K T S', 'K AA1 N V IH0 K T S', 'V\\\\\\n']\n",
      "['COORDINATE', 'K OW0 AO1 R D AH0 N EY2 T', 'K OW0 AO1 R D AH0 N AH0 T', 'V\\\\\\n']\n",
      "['COORDINATES', 'K OW0 AO1 R D AH0 N EY2 T S', 'K OW0 AO1 R D AH0 N AH0 T S', 'V\\\\\\n']\n",
      "['COUNTERBALANCE', 'K AW1 N T ER0 B AE2 L AH0 N S', 'K AW2 N T ER0 B AE1 L AH0 N S', 'V\\\\\\n']\n",
      "['COUNTERBALANCES', 'K AW2 N T ER0 B AE1 L AH0 N S IH0 Z', 'K AW1 N T ER0 B AE2 L AH0 N S IH0 Z', 'V\\\\\\n']\n",
      "['CRABBED', 'K R AE1 B D', 'K R AE1 B IH0 D', 'V\\\\\\n']\n",
      "['CROOKED', 'K R UH1 K T', 'K R UH1 K AH0 D', 'V\\\\\\n']\n",
      "['CURATE', 'K Y UH0 R AH1 T', 'K Y UH1 R AH0 T', 'V\\\\\\n']\n",
      "['CURSED', 'K ER1 S T', 'K ER1 S IH0 D', 'V\\\\\\n']\n",
      "['DECOY', 'D IY0 K OY1', 'D IY1 K OY0', 'V\\\\\\n']\n",
      "['DECOYS', 'D IY0 K OY1 Z', 'D IY1 K OY0 Z', 'V\\\\\\n']\n",
      "['DECREASE', 'D IH0 K R IY1 S', 'D IY1 K R IY2 S', 'V\\\\\\n']\n",
      "['DECREASES', 'D IH0 K R IY1 S IH0 Z', 'D IY1 K R IY2 S IH0 Z', 'V\\\\\\n']\n",
      "['DEFECT', 'D IH0 F EH1 K T', 'D IY1 F EH0 K T', 'V\\\\\\n']\n",
      "['DEFECTS', 'D IH0 F EH1 K T S', 'D IY1 F EH0 K T S', 'V\\\\\\n']\n",
      "['DEGENERATE', 'D IH0 JH EH1 N ER0 EY2 T', 'D IH0 JH EH1 N ER0 AH0 T', 'V\\\\\\n']\n",
      "['DEGENERATES', 'D IH0 JH EH1 N ER0 EY2 T S', 'D IH0 JH EH1 N ER0 AH0 T S', 'V\\\\\\n']\n",
      "['DELEGATE', 'D EH1 L AH0 G EY2 T', 'D EH1 L AH0 G AH0 T', 'V\\\\\\n']\n",
      "['DELEGATES', 'D EH1 L AH0 G EY2 T S', 'D EH1 L AH0 G AH0 T S', 'V\\\\\\n']\n",
      "['DELIBERATE', 'D IH0 L IH1 B ER0 EY2 T', 'D IH0 L IH1 B ER0 AH0 T', 'V\\\\\\n']\n",
      "['DESERT', 'D IH0 Z ER1 T', 'D EH1 Z ER0 T', 'V\\\\\\n']\n",
      "['DESERTS', 'D IH0 Z ER1 T S', 'D EH1 Z ER0 T S', 'V\\\\\\n']\n",
      "['DESOLATE', 'D EH1 S AH0 L EY2 T', 'D EH1 S AH0 L AH0 T', 'V\\\\\\n']\n",
      "['DIAGNOSES', 'D AY1 AH0 G N OW2 Z IY0 Z', 'D AY2 AH0 G N OW1 S IY0 Z', 'V\\\\\\n']\n",
      "['DICTATE', 'D IH0 K T EY1 T', 'D IH1 K T EY2 T', 'V\\\\\\n']\n",
      "['DICTATES', 'D IH0 K T EY1 T S', 'D IH1 K T EY2 T S', 'V\\\\\\n']\n",
      "['DIFFUSE', 'D IH0 F Y UW1 Z', 'D IH0 F Y UW1 S', 'V\\\\\\n']\n",
      "['DIGEST', 'D AY0 JH EH1 S T', 'D AY1 JH EH0 S T', 'V\\\\\\n']\n",
      "['DIGESTS', 'D AY2 JH EH1 S T S', 'D AY1 JH EH0 S T S', 'V\\\\\\n']\n",
      "['DISCARD', 'D IH0 S K AA1 R D', 'D IH1 S K AA0 R D', 'V\\\\\\n']\n",
      "['DISCARDS', 'D IH0 S K AA1 R D Z', 'D IH1 S K AA0 R D Z', 'V\\\\\\n']\n",
      "['DISCHARGE', 'D IH0 S CH AA1 R JH', 'D IH1 S CH AA2 R JH', 'V\\\\\\n']\n",
      "['DISCHARGES', 'D IH0 S CH AA1 R JH AH0 Z', 'D IH1 S CH AA2 R JH AH0 Z', 'V\\\\\\n']\n",
      "['DISCOUNT', 'D IH0 S K AW1 N T', 'D IH1 S K AW0 N T', 'V\\\\\\n']\n",
      "['DISCOUNTS', 'D IH0 S K AW1 N T S', 'D IH1 S K AW2 N T S', 'V\\\\\\n']\n",
      "['DISCOURSE', 'D IH0 S K AO1 R S', 'D IH1 S K AO0 R S', 'V\\\\\\n']\n",
      "['DISCOURSES', 'D IH0 S K AO1 R S IH0 Z', 'D IH1 S K AO0 R S IH0 Z', 'V\\\\\\n']\n",
      "['DOCUMENT', 'D AA1 K Y UW0 M EH0 N T', 'D AA1 K Y AH0 M AH0 N T', 'V\\\\\\n']\n",
      "['DOCUMENTS', 'D AA1 K Y UW0 M EH0 N T S', 'D AA1 K Y AH0 M AH0 N T S', 'V\\\\\\n']\n",
      "['DOGGED', 'D AO1 G IH0 D', 'D AO1 G D', 'V\\\\\\n']\n",
      "['DUPLICATE', 'D UW1 P L AH0 K EY2 T', 'D UW1 P L AH0 K AH0 T', 'V\\\\\\n']\n",
      "['DUPLICATES', 'D UW1 P L AH0 K EY2 T S', 'D UW1 P L AH0 K AH0 T S', 'V\\\\\\n']\n",
      "['EJACULATE', 'IH0 JH AE1 K Y UW0 L EY2 T', 'IH0 JH AE1 K Y UW0 L AH0 T', 'V\\\\\\n']\n",
      "['EJACULATES', 'IH0 JH AE1 K Y UW0 L EY2 T S', 'IH0 JH AE1 K Y UW0 L AH0 T S', 'V\\\\\\n']\n",
      "['ELABORATE', 'IH0 L AE1 B ER0 EY2 T', 'IH0 L AE1 B R AH0 T', 'V\\\\\\n']\n",
      "['ENTRANCE', 'IH0 N T R AH1 N S', 'EH1 N T R AH0 N S', 'V\\\\\\n']\n",
      "['ENTRANCES', 'IH0 N T R AH1 N S AH0 Z', 'EH1 N T R AH0 N S AH0 Z', 'V\\\\\\n']\n",
      "['ENVELOPE', 'IH0 N V EH1 L AH0 P', 'EH1 N V AH0 L OW2 P', 'V\\\\\\n']\n",
      "['ENVELOPES', 'IH0 N V EH1 L AH0 P S', 'EH1 N V AH0 L OW2 P S', 'V\\\\\\n']\n",
      "['ESCORT', 'EH0 S K AO1 R T', 'EH1 S K AO0 R T', 'V\\\\\\n']\n",
      "['ESCORTS', 'EH0 S K AO1 R T S', 'EH1 S K AO0 R T S', 'V\\\\\\n']\n",
      "['ESSAY', 'EH0 S EY1', 'EH1 S EY2', 'V\\\\\\n']\n",
      "['ESSAYS', 'EH0 S EY1 Z', 'EH1 S EY2 Z', 'V\\\\\\n']\n",
      "['ESTIMATE', 'EH1 S T AH0 M EY2 T', 'EH1 S T AH0 M AH0 T', 'V\\\\\\n']\n",
      "['ESTIMATES', 'EH1 S T AH0 M EY2 T S', 'EH1 S T AH0 M AH0 T S', 'V\\\\\\n']\n",
      "['EXCESS', 'IH0 K S EH1 S', 'EH1 K S EH2 S', 'V\\\\\\n']\n",
      "['EXCISE', 'EH0 K S AY1 S', 'EH1 K S AY0 Z', 'V\\\\\\n']\n",
      "['EXCUSE', 'IH0 K S K Y UW1 Z', 'IH0 K S K Y UW1 S', 'V\\\\\\n']\n",
      "['EXCUSES', 'IH0 K S K Y UW1 Z IH0 Z', 'IH0 K S K Y UW1 S IH0 Z', 'V\\\\\\n']\n",
      "['EXPATRIATE', 'EH0 K S P EY1 T R IY0 EY2 T', 'EH0 K S P EY1 T R IY0 AH0 T', 'V\\\\\\n']\n",
      "['EXPATRIATES', 'EH0 K S P EY1 T R IY0 EY2 T S', 'EH0 K S P EY1 T R IY0 AH0 T S', 'V\\\\\\n']\n",
      "['EXPLOIT', 'EH1 K S P L OY2 T', 'EH2 K S P L OY1 T', 'V\\\\\\n']\n",
      "['EXPLOITS', 'EH1 K S P L OY2 T S', 'EH2 K S P L OY1 T S', 'V\\\\\\n']\n",
      "['EXPORT', 'IH0 K S P AO1 R T', 'EH1 K S P AO0 R T', 'V\\\\\\n']\n",
      "['EXPORTS', 'IH0 K S P AO1 R T S', 'EH1 K S P AO0 R T S', 'V\\\\\\n']\n",
      "['EXTRACT', 'IH0 K S T R AE1 K T', 'EH1 K S T R AE2 K T', 'V\\\\\\n']\n",
      "['EXTRACTS', 'IH0 K S T R AE1 K T S', 'EH1 K S T R AE2 K T S', 'V\\\\\\n']\n",
      "['FERMENT', 'F ER0 M EH1 N T', 'F ER1 M EH0 N T', 'V\\\\\\n']\n",
      "['FERMENTS', 'F ER0 M EH1 N T S', 'F ER1 M EH0 N T S', 'V\\\\\\n']\n",
      "['FRAGMENT', 'F R AE1 G M AH0 N T', 'F R AE0 G M EH1 N T', 'V\\\\\\n']\n",
      "['FRAGMENTS', 'F R AE0 G M EH1 N T S', 'F R AE1 G M AH0 N T S', 'V\\\\\\n']\n",
      "['FREQUENT', 'F R IY1 K W EH2 N T', 'F R IY1 K W AH0 N T', 'V\\\\\\n']\n",
      "['GRADUATE', 'G R AE1 JH AH0 W EY2 T', 'G R AE1 JH AH0 W AH0 T', 'V\\\\\\n']\n",
      "['GRADUATES', 'G R AE1 JH AH0 W EY2 T S', 'G R AE1 JH AH0 W AH0 T S', 'V\\\\\\n']\n",
      "['HOUSE', 'HH AW1 Z', 'HH AW1 S', 'V\\\\\\n']\n",
      "['IMPACT', 'IH2 M P AE1 K T', 'IH1 M P AE0 K T', 'V\\\\\\n']\n",
      "['IMPACTS', 'IH2 M P AE1 K T S', 'IH1 M P AE0 K T S', 'V\\\\\\n']\n",
      "['IMPLANT', 'IH2 M P L AE1 N T', 'IH1 M P L AE2 N T', 'V\\\\\\n']\n",
      "['IMPLANTS', 'IH2 M P L AE1 N T S', 'IH1 M P L AE2 N T S', 'V\\\\\\n']\n",
      "['IMPLEMENT', 'IH1 M P L AH0 M EH0 N T', 'IH1 M P L AH0 M AH0 N T', 'V\\\\\\n']\n",
      "['IMPLEMENTS', 'IH1 M P L AH0 M EH0 N T S', 'IH1 M P L AH0 M AH0 N T S', 'V\\\\\\n']\n",
      "['IMPORT', 'IH2 M P AO1 R T', 'IH1 M P AO2 R T', 'V\\\\\\n']\n",
      "['IMPORTS', 'IH2 M P AO1 R T S', 'IH1 M P AO2 R T S', 'V\\\\\\n']\n",
      "['IMPRESS', 'IH0 M P R EH1 S', 'IH1 M P R EH0 S', 'V\\\\\\n']\n",
      "['IMPRINT', 'IH1 M P R IH0 N T', 'IH2 M P R IH1 N T', 'V\\\\\\n']\n",
      "['IMPRINTS', 'IH2 M P R IH1 N T S', 'IH1 M P R IH0 N T S', 'V\\\\\\n']\n",
      "['INCENSE', 'IH2 N S EH1 N S', 'IH1 N S EH2 N S', 'V\\\\\\n']\n",
      "['INCLINE', 'IH2 N K L AY1 N', 'IH1 N K L AY0 N', 'V\\\\\\n']\n",
      "['INCLINES', 'IH2 N K L AY1 N Z', 'IH1 N K L AY0 N Z', 'V\\\\\\n']\n",
      "['INCORPORATE', 'IH2 N K AO1 R P ER0 EY2 T', 'IH2 N K AO1 R P ER0 AH0 T', 'V\\\\\\n']\n",
      "['INCREASE', 'IH2 N K R IY1 S', 'IH1 N K R IY2 S', 'V\\\\\\n']\n",
      "['INCREASES', 'IH2 N K R IY1 S IH0 Z', 'IH1 N K R IY2 S IH0 Z', 'V\\\\\\n']\n",
      "['INDENT', 'IH2 N D EH1 N T', 'IH1 N D EH0 N T', 'V\\\\\\n']\n",
      "['INDENTS', 'IH2 N D EH1 N T S', 'IH1 N D EH0 N T S', 'V\\\\\\n']\n",
      "['INEBRIATE', 'IH2 N EH1 B R IY0 EY2 T', 'IH2 N EH1 B R IY0 AH0 T', 'V\\\\\\n']\n",
      "['INEBRIATES', 'IH2 N EH1 B R IY0 EY2 T S', 'IH2 N EH1 B R IY0 AH0 T S', 'V\\\\\\n']\n",
      "['INITIATE', 'IH2 N IH1 SH IY0 EY2 T', 'IH2 N IH1 SH IY0 AH0 T', 'V\\\\\\n']\n",
      "['INITIATES', 'IH2 N IH1 SH IY0 EY2 T S', 'IH2 N IH1 SH IY0 AH0 T S', 'V\\\\\\n']\n",
      "['INLAY', 'IH2 N L EY1', 'IH1 N L EY2', 'V\\\\\\n']\n",
      "['INLAYS', 'IH2 N L EY1 Z', 'IH1 N L EY2 Z', 'V\\\\\\n']\n",
      "['INSERT', 'IH2 N S ER1 T', 'IH1 N S ER2 T', 'V\\\\\\n']\n",
      "['INSERTS', 'IH2 N S ER1 T S', 'IH1 N S ER2 T S', 'V\\\\\\n']\n",
      "['INSET', 'IH2 N S EH1 T', 'IH1 N S EH2 T', 'V\\\\\\n']\n",
      "['INSETS', 'IH2 N S EH1 T S', 'IH1 N S EH2 T S', 'V\\\\\\n']\n",
      "['INSTINCT', 'IH2 N S T IH1 NG K T', 'IH1 N S T IH0 NG K T', 'V\\\\\\n']\n",
      "['INSULT', 'IH2 N S AH1 L T', 'IH1 N S AH2 L T', 'V\\\\\\n']\n",
      "['INSULTS', 'IH2 N S AH1 L T S', 'IH1 N S AH2 L T S', 'V\\\\\\n']\n",
      "['INTERCHANGE', 'IH2 T ER0 CH EY1 N JH', 'IH1 N T ER0 CH EY2 N JH', 'V\\\\\\n']\n",
      "['INTERCHANGES', 'IH2 T ER0 CH EY1 N JH IH0 Z', 'IH1 N T ER0 CH EY2 N JH IH0 Z', 'V\\\\\\n']\n",
      "['INTERDICT', 'IH2 N T ER0 D IH1 K T', 'IH1 N T ER0 D IH2 K T', 'V\\\\\\n']\n",
      "['INTERDICTS', 'IH2 N T ER0 D IH1 K T S', 'IH1 N T ER0 D IH2 K T S', 'V\\\\\\n']\n",
      "['INTERN', 'IH0 N T ER1 N', 'IH1 N T ER0 N', 'V\\\\\\n']\n",
      "['INTERNS', 'IH0 N T ER1 N Z', 'IH1 N T ER0 N Z', 'V\\\\\\n']\n",
      "['INTIMATE', 'IH1 N T IH0 M EY2 T', 'IH1 N T AH0 M AH0 T', 'V\\\\\\n']\n",
      "['INTIMATES', 'IH1 N T IH0 M EY2 T S', 'IH1 N T AH0 M AH0 T S', 'V\\\\\\n']\n",
      "['INTROVERT', 'IH2 N T R AO0 V ER1 T', 'IH1 N T R AO0 V ER2 T', 'V\\\\\\n']\n",
      "['INTROVERTS', 'IH2 N T R AO0 V ER1 T S', 'IH1 N T R AO0 V ER2 T S', 'V\\\\\\n']\n",
      "['INVERSE', 'IH1 N V ER0 S', 'IH2 N V ER1 S', 'V\\\\\\n']\n",
      "['INVITE', 'IH2 N V AY1 T', 'IH1 N V AY0 T', 'V\\\\\\n']\n",
      "['INVITES', 'IH2 N V AY1 T S', 'IH1 N V AY0 T S', 'V\\\\\\n']\n",
      "['JAGGED', 'JH AE1 G D', 'JH AE1 G IH0 D', 'V\\\\\\n']\n",
      "['LEARNED', 'L ER1 N IH0 D', 'L ER1 N D', 'V\\\\\\n']\n",
      "['LEGITIMATE', 'L AH0 JH IH1 T AH0 M EY2 T', 'L AH0 JH IH1 T AH0 M AH0 T', 'V\\\\\\n']\n",
      "['MANDATE', 'M AE1 N D EY2 T', 'M AE2 N D EY1 T', 'V\\\\\\n']\n",
      "['MISCONDUCT', 'M IH2 S K AA1 N D AH0 K T', 'M IH2 S K AA0 N D AH1 K T', 'V\\\\\\n']\n",
      "['MISPRINT', 'M IH2 S P R IH1 N T', 'M IH1 S P R IH0 N T', 'V\\\\\\n']\n",
      "['MISPRINTS', 'M IH2 S P R IH1 N T S', 'M IH1 S P R IH0 N T S', 'V\\\\\\n']\n",
      "['MISUSE', 'M IH0 S Y UW1 S', 'M IH0 S Y UW1 Z', 'V\\\\\\n']\n",
      "['MISUSES', 'M IH0 S Y UW1 Z IH0 Z', 'M IH0 S Y UW1 S IH0 Z', 'V\\\\\\n']\n",
      "['MODERATE', 'M AA1 D ER0 EY2 T', 'M AA1 D ER0 AH0 T', 'V\\\\\\n']\n",
      "['MODERATES', 'M AA1 D ER0 EY2 T S', 'M AA1 D ER0 AH0 T S', 'V\\\\\\n']\n",
      "['MOUTH', 'M AW1 TH', 'M AW1 DH', 'V\\\\\\n']\n",
      "['MOUTHS', 'M AW1 DH Z', 'M AW1 TH S', 'V\\\\\\n']\n",
      "['OBJECT', 'AA1 B JH EH0 K T', 'AH0 B JH EH1 K T', 'V\\\\\\n']\n",
      "['OBJECTS', 'AH0 B JH EH1 K T S', 'AA1 B JH EH0 K T S', 'V\\\\\\n']\n",
      "['ORNAMENT', 'AO1 R N AH0 M EH0 N T', 'AO1 R N AH0 M AH0 N T', 'V\\\\\\n']\n",
      "['ORNAMENTS', 'AO1 R N AH0 M EH0 N T S', 'AO1 R N AH0 M AH0 N T S', 'V\\\\\\n']\n",
      "['OVERCHARGE', 'OW2 V ER0 CH AA1 R JH', 'OW1 V ER0 CH AA2 R JH', 'V\\\\\\n']\n",
      "['OVERCHARGES', 'OW2 V ER0 CH AA1 R JH IH0 Z', 'OW1 V ER0 CH AA2 R JH IH0 Z', 'V\\\\\\n']\n",
      "['OVERFLOW', 'OW2 V ER0 F L OW1', 'OW1 V ER0 F L OW2', 'V\\\\\\n']\n",
      "['OVERFLOWS', 'OW2 V ER0 F L OW1 Z', 'OW1 V ER0 F L OW2 Z', 'V\\\\\\n']\n",
      "['OVERHANG', 'OW2 V ER0 HH AE1 NG', 'OW1 V ER0 HH AE2 NG', 'V\\\\\\n']\n",
      "['OVERHANGS', 'OW2 V ER0 HH AE1 NG Z', 'OW1 V ER0 HH AE2 NG Z', 'V\\\\\\n']\n",
      "['OVERHAUL', 'OW2 V ER0 HH AO1 L', 'OW1 V ER0 HH AO2 L', 'V\\\\\\n']\n",
      "['OVERHAULS', 'OW2 V ER0 HH AO1 L Z', 'OW1 V ER0 HH AO2 L Z', 'V\\\\\\n']\n",
      "['OVERLAP', 'OW2 V ER0 L AE1 P', 'OW1 V ER0 L AE2 P', 'V\\\\\\n']\n",
      "['OVERLAPS', 'OW2 V ER0 L AE1 P S', 'OW1 V ER0 L AE2 P S', 'V\\\\\\n']\n",
      "['OVERLAY', 'OW2 V ER0 L EY1', 'OW1 V ER0 L EY2', 'V\\\\\\n']\n",
      "['OVERLAYS', 'OW2 V ER0 L EY1 Z', 'OW1 V ER0 L EY2 Z', 'V\\\\\\n']\n",
      "['OVERWORK', 'OW2 V ER0 W ER1 K', 'OW1 V ER0 W ER2 K', 'V\\\\\\n']\n",
      "['PERFECT', 'P ER0 F EH1 K T', 'P ER1 F IH2 K T', 'V\\\\\\n']\n",
      "['PERFUME', 'P ER0 F Y UW1 M', 'P ER1 F Y UW0 M', 'V\\\\\\n']\n",
      "['PERFUMES', 'P ER0 F Y UW1 M Z', 'P ER1 F Y UW0 M Z', 'V\\\\\\n']\n",
      "['PERMIT', 'P ER0 M IH1 T', 'P ER1 M IH2 T', 'V\\\\\\n']\n",
      "['PERMITS', 'P ER0 M IH1 T S', 'P ER1 M IH2 T S', 'V\\\\\\n']\n",
      "['PERVERT', 'P ER0 V ER1 T', 'P ER1 V ER0 T', 'V\\\\\\n']\n",
      "['PERVERTS', 'P ER0 V ER1 T S', 'P ER1 V ER0 T S', 'V\\\\\\n']\n",
      "['PONTIFICATE', 'P AA0 N T IH1 F AH0 K AH0 T', 'P AA0 N T IH1 F AH0 K EY2 T', 'V\\\\\\n']\n",
      "['PONTIFICATES', 'P AA0 N T IH1 F AH0 K EY2 T S', 'P AA0 N T IH1 F AH0 K AH0 T S', 'V\\\\\\n']\n",
      "['PRECIPITATE', 'P R IH0 S IH1 P IH0 T AH0 T', 'P R IH0 S IH1 P IH0 T EY2 T', 'V\\\\\\n']\n",
      "['PREDICATE', 'P R EH1 D IH0 K AH0 T', 'P R EH1 D AH0 K EY2 T', 'V\\\\\\n']\n",
      "['PREDICATES', 'P R EH1 D AH0 K EY2 T S', 'P R EH1 D IH0 K AH0 T S', 'V\\\\\\n']\n",
      "['PREFIX', 'P R IY2 F IH1 K S', 'P R IY1 F IH0 K S', 'V\\\\\\n']\n",
      "['PREFIXES', 'P R IY2 F IH1 K S IH0 JH', 'P R IY1 F IH0 K S IH0 JH', 'V\\\\\\n']\n",
      "['PRESAGE', 'P R EH2 S IH1 JH', 'P R EH1 S IH0 JH', 'V\\\\\\n']\n",
      "['PRESAGES', 'P R EH2 S IH1 JH IH0 JH', 'P R EH1 S IH0 JH IH0 JH', 'V\\\\\\n']\n",
      "['PRESENT', 'P R IY0 Z EH1 N T', 'P R EH1 Z AH0 N T', 'V\\\\\\n']\n",
      "['PRESENTS', 'P R IY0 Z EH1 N T S', 'P R EH1 Z AH0 N T S', 'V\\\\\\n']\n",
      "['PROCEEDS', 'P R AH0 S IY1 D Z', 'P R OW1 S IY0 D Z', 'V\\\\\\n']\n",
      "['PROCESS', 'P R AO2 S EH1 S', 'P R AA1 S EH2 S', 'V\\\\\\n']\n",
      "['PROCESSES', 'P R AA1 S EH0 S AH0 Z', 'P R AO2 S EH1 S AH0 Z', 'V\\\\\\n']\n",
      "['PROCESSING', 'P R AA0 S EH1 S IH0 NG', 'P R AA1 S EH0 S IH0 NG', 'V\\\\\\n']\n",
      "['PRODUCE', 'P R AH0 D UW1 S', 'P R OW1 D UW0 S', 'V\\\\\\n']\n",
      "['PROGRESS', 'P R AH0 G R EH1 S', 'P R AA1 G R EH2 S', 'V\\\\\\n']\n",
      "['PROGRESSES', 'P R OW0 G R EH1 S AH0 Z', 'P R AA1 G R EH2 S AH0 Z', 'V\\\\\\n']\n",
      "['PROJECT', 'P R AA0 JH EH1 K T', 'P R AA1 JH EH0 K T', 'V\\\\\\n']\n",
      "['PROJECTS', 'P R AA0 JH EH1 K T S', 'P R AA1 JH EH0 K T S', 'V\\\\\\n']\n",
      "['PROSPECT', 'P R AH2 S P EH1 K T', 'P R AA1 S P EH0 K T', 'V\\\\\\n']\n",
      "['PROSPECTS', 'P R AH2 S P EH1 K T S', 'P R AA1 S P EH0 K T S', 'V\\\\\\n']\n",
      "['PROSTRATE', 'P R AA0 S T R EY1 T', 'P R AA1 S T R EY0 T', 'V\\\\\\n']\n",
      "['PROTEST', 'P R AH0 T EH1 S T', 'P R OW1 T EH2 S T', 'V\\\\\\n']\n",
      "['PROTESTS', 'P R AH0 T EH1 S T S', 'P R OW1 T EH2 S T S', 'V\\\\\\n']\n",
      "['PURPORT', 'P ER0 P AO1 R T', 'P ER1 P AO2 R T', 'V\\\\\\n']\n",
      "['QUADRUPLE', 'K W AA1 D R UW0 P AH0 L', 'K W AA0 D R UW1 P AH0 L', 'V\\\\\\n']\n",
      "['QUADRUPLES', 'K W AA0 D R UW1 P AH0 L Z', 'K W AA1 D R UW0 P AH0 L Z', 'V\\\\\\n']\n",
      "['RAGGED', 'R AE1 G D', 'R AE1 G AH0 D', 'V\\\\\\n']\n",
      "['RAMPAGE', 'R AE2 M P EY1 JH', 'R AE1 M P EY2 JH', 'V\\\\\\n']\n",
      "['RAMPAGES', 'R AE2 M P EY1 JH IH0 Z', 'R AE1 M P EY2 JH IH0 Z', 'V\\\\\\n']\n",
      "['READ', 'R IY1 D', 'R EH1 D', 'VBD\\\\\\n']\n",
      "['REBEL', 'R EH1 B AH0 L', 'R IH0 B EH1 L', 'V\\\\\\n']\n",
      "['REBELS', 'R IH0 B EH1 L Z', 'R EH1 B AH0 L Z', 'V\\\\\\n']\n",
      "['REBOUND', 'R IY0 B AW1 N D', 'R IY1 B AW0 N D', 'V\\\\\\n']\n",
      "['REBOUNDS', 'R IY0 B AW1 N D Z', 'R IY1 B AW0 N D Z', 'V\\\\\\n']\n",
      "['RECALL', 'R IH0 K AO1 L', 'R IY1 K AO2 L', 'V\\\\\\n']\n",
      "['RECALLS', 'R IH0 K AO1 L Z', 'R IY1 K AO2 L Z', 'V\\\\\\n']\n",
      "['RECAP', 'R IH0 K AE1 P', 'R IY1 K AE2 P', 'V\\\\\\n']\n",
      "['RECAPPED', 'R IH0 K AE1 P T', 'R IY1 K AE2 P T', 'V\\\\\\n']\n",
      "['RECAPPING', 'R IH0 K AE1 P IH0 NG', 'R IY1 K AE2 P IH0 NG', 'V\\\\\\n']\n",
      "['RECAPS', 'R IH0 K AE1 P S', 'R IY1 K AE2 P S', 'V\\\\\\n']\n",
      "['RECOUNT', 'R IY2 K AW1 N T', ' R IH1 K AW0 N T', 'V\\\\\\n']\n",
      "['RECOUNTS', 'R IY2 K AW1 N T S', ' R IH1 K AW0 N T S', 'V\\\\\\n']\n",
      "['RECORD', 'R IH0 K AO1 R D', 'R EH1 K ER0 D', 'V\\\\\\n']\n",
      "['RECORDS', 'R IH0 K AO1 R D Z', 'R EH1 K ER0 D Z', 'V\\\\\\n']\n",
      "['REFILL', 'R IY0 F IH1 L', 'R IY1 F IH0 L', 'V\\\\\\n']\n",
      "['REFILLS', 'R IY0 F IH1 L Z', 'R IY1 F IH0 L Z', 'V\\\\\\n']\n",
      "['REFIT', 'R IY0 F IH1 T', 'R IY1 F IH0 T', 'V\\\\\\n']\n",
      "['REFITS', 'R IY0 F IH1 T S', 'R IY1 F IH0 T S', 'V\\\\\\n']\n",
      "['REFRESH', 'R IH0 F R EH1 SH', 'R IH1 F R EH0 SH', 'V\\\\\\n']\n",
      "['REFUND', 'R IH0 F AH1 N D', 'R IY1 F AH2 N D', 'V\\\\\\n']\n",
      "['REFUNDS', 'R IH0 F AH1 N D Z', 'R IY1 F AH2 N D Z', 'V\\\\\\n']\n",
      "['REFUSE', 'R IH0 F Y UW1 Z', 'R EH1 F Y UW2 Z', 'V\\\\\\n']\n",
      "['REGENERATE', 'R IY0 JH EH1 N ER0 EY2 T', 'R IY0 JH EH1 N ER0 AH0 T', 'V\\\\\\n']\n",
      "['REHASH', 'R IY0 HH AE1 SH', 'R IY1 HH AE0 SH', 'V\\\\\\n']\n",
      "['REHASHES', 'R IY0 HH AE1 SH IH0 Z', 'R IY1 HH AE0 SH IH0 Z', 'V\\\\\\n']\n",
      "['REINCARNATE', 'R IY2 IH0 N K AA1 R N EY2 T', 'R IY2 IH0 N K AA1 R N AH0 T', 'V\\\\\\n']\n",
      "['REJECT', 'R IH0 JH EH1 K T', 'R IY1 JH EH0 K T', 'V\\\\\\n']\n",
      "['REJECTS', 'R IH0 JH EH1 K T S', 'R IY1 JH EH0 K T S', 'V\\\\\\n']\n",
      "['RELAY', 'R IY2 L EY1', 'R IY1 L EY2', 'V\\\\\\n']\n",
      "['RELAYING', 'R IY2 L EY1 IH0 NG', 'R IY1 L EY2 IH0 NG', 'V\\\\\\n']\n",
      "['RELAYS', 'R IY2 L EY1 Z', 'R IY1 L EY2 Z', 'V\\\\\\n']\n",
      "['REMAKE', 'R IY2 M EY1 K', 'R IY1 M EY0 K', 'V\\\\\\n']\n",
      "['REMAKES', 'R IY2 M EY1 K S', 'R IY1 M EY0 K S', 'V\\\\\\n']\n",
      "['REPLAY', 'R IY0 P L EY1', 'R IY1 P L EY0', 'V\\\\\\n']\n",
      "['REPLAYS', 'R IY0 P L EY1 Z', 'R IY1 P L EY0 Z', 'V\\\\\\n']\n",
      "['REPRINT', 'R IY0 P R IH1 N T', 'R IY1 P R IH0 N T', 'V\\\\\\n']\n",
      "['REPRINTS', 'R IY0 P R IH1 N T S', 'R IY1 P R IH0 N T S', 'V\\\\\\n']\n",
      "['RERUN', 'R IY2 R AH1 N', 'R IY1 R AH0 N', 'V\\\\\\n']\n",
      "['RERUNS', 'R IY2 R AH1 N Z', 'R IY1 R AH0 N Z', 'V\\\\\\n']\n",
      "['RESUME', 'R IY0 Z UW1 M', 'R EH1 Z AH0 M EY2', 'V\\\\\\n']\n",
      "['RETAKE', 'R IY0 T EY1 K', 'R IY1 T EY0 K', 'V\\\\\\n']\n",
      "['RETAKES', 'R IY0 T EY1 K S', 'R IY1 T EY0 K S', 'V\\\\\\n']\n",
      "['RETHINK', 'R IY2 TH IH1 NG K', 'R IY1 TH IH0 NG K', 'V\\\\\\n']\n",
      "['RETHINKS', 'R IY2 TH IH1 NG K S', 'R IY1 TH IH0 NG K S', 'V\\\\\\n']\n",
      "['RETREAD', 'R IY2 T R EH1 D', 'R IY1 T R EH0 D', 'V\\\\\\n']\n",
      "['RETREADS', 'R IY2 T R EH1 D Z', 'R IY1 T R EH0 D Z', 'V\\\\\\n']\n",
      "['REWRITE', 'R IY0 R AY1 T', 'R IY1 R AY2 T', 'V\\\\\\n']\n",
      "['REWRITES', 'R IY0 R AY1 T S', 'R IY1 R AY2 T S', 'V\\\\\\n']\n",
      "['SEGMENT', 'S EH1 G M AH0 N T', 'S EH2 G M EH1 N T', 'V\\\\\\n']\n",
      "['SEGMENTS', 'S EH2 G M EH1 N T S', 'S EH1 G M AH0 N T S', 'V\\\\\\n']\n",
      "['SEPARATE', 'S EH1 P ER0 EY2 T', 'S EH1 P ER0 IH0 T', 'V\\\\\\n']\n",
      "['SEPARATES', 'S EH1 P ER0 EY2 T S', 'S EH1 P ER0 IH0 T S', 'V\\\\\\n']\n",
      "['SUBCONTRACT', 'S AH0 B K AA1 N T R AE2 K T', 'S AH2 B K AA0 N T R AE1 K T', 'V\\\\\\n']\n",
      "['SUBCONTRACTS', 'S AH2 B K AA0 N T R AE1 K T S', 'S AH0 B K AA1 N T R AE2 K T S', 'V\\\\\\n']\n",
      "['SUBJECT', 'S AH0 B JH EH1 K T', 'S AH1 B JH IH0 K T', 'V\\\\\\n']\n",
      "['SUBJECTS', 'S AH0 B JH EH1 K T S', 'S AH1 B JH IH0 K T S', 'V\\\\\\n']\n",
      "['SUBORDINATE', 'S AH0 B AO1 R D AH0 N EY2 T', 'S AH0 B AO1 R D AH0 N AH0 T', 'V\\\\\\n']\n",
      "['SUBORDINATES', 'S AH0 B AO1 R D AH0 N EY2 T S', 'S AH0 B AO1 R D AH0 N AH0 T S', 'V\\\\\\n']\n",
      "['SUPPLEMENT', 'S AH1 P L AH0 M EH0 N T', 'S AH1 P L AH0 M AH0 N T', 'V\\\\\\n']\n",
      "['SUPPLEMENTS', 'S AH1 P L AH0 M EH0 N T S', 'S AH1 P L AH0 M AH0 N T S', 'V\\\\\\n']\n",
      "['SURMISE', 'S ER0 M AY1 Z', 'S ER1 M AY0 Z', 'V\\\\\\n']\n",
      "['SURMISES', 'S ER0 M AY1 Z IH0 Z', 'S ER1 M AY0 Z IH0 Z', 'V\\\\\\n']\n",
      "['SURVEY', 'S ER0 V EY1', 'S ER1 V EY2', 'V\\\\\\n']\n",
      "['SURVEYS', 'S ER0 V EY1 Z', 'S ER1 V EY2 Z', 'V\\\\\\n']\n",
      "['SUSPECT', 'S AH0 S P EH1 K T', 'S AH1 S P EH2 K T', 'V\\\\\\n']\n",
      "['SUSPECTS', 'S AH0 S P EH1 K T S', 'S AH1 S P EH2 K T S', 'V\\\\\\n']\n",
      "['SYNDICATE', 'S IH1 N D AH0 K EY2 T', 'S IH1 N D IH0 K AH0 T', 'V\\\\\\n']\n",
      "['SYNDICATES', 'S IH1 N D IH0 K EY2 T S', 'S IH1 N D IH0 K AH0 T S', 'V\\\\\\n']\n",
      "['TORMENT', 'T AO1 R M EH2 N T', 'T AO0 R M EH1 N T', 'V\\\\\\n']\n",
      "['TRANSFER', 'T R AE0 N S F ER1', 'T R AE1 N S F ER0', 'V\\\\\\n']\n",
      "['TRANSFERS', 'T R AE0 N S F ER1 Z', 'T R AE1 N S F ER0 Z', 'V\\\\\\n']\n",
      "['TRANSPLANT', 'T R AE0 N S P L AE1 N T', 'T R AE1 N S P L AE0 N T', 'V\\\\\\n']\n",
      "['TRANSPLANTS', 'T R AE0 N S P L AE1 N T S', 'T R AE1 N S P L AE0 N T S', 'V\\\\\\n']\n",
      "['TRANSPORT', 'T R AE0 N S P AO1 R T', 'T R AE1 N S P AO0 R T', 'V\\\\\\n']\n",
      "['TRANSPORTS', 'T R AE0 N S P AO1 R T S', 'T R AE1 N S P AO0 R T S', 'V\\\\\\n']\n",
      "['TRIPLICATE', 'T R IH1 P L IH0 K EY2 T', 'T R IH1 P L IH0 K AH0 T', 'V\\\\\\n']\n",
      "['TRIPLICATES', 'T R IH1 P L IH0 K EY2 T S', 'T R IH1 P L IH0 K AH0 T S', 'V\\\\\\n']\n",
      "['UNDERCUT', 'AH2 N D ER0 K AH1 T', 'AH1 N D ER0 K AH2 T', 'V\\\\\\n']\n",
      "['UNDERESTIMATE', 'AH1 N D ER0 EH1 S T AH0 M EY2 T', 'AH1 N D ER0 EH1 S T AH0 M AH0 T', 'V\\\\\\n']\n",
      "['UNDERESTIMATES', 'AH1 N D ER0 EH1 S T AH0 M EY2 T S', 'AH1 N D ER0 EH1 S T AH0 M AH0 T S', 'V\\\\\\n']\n",
      "['UNDERLINE', 'AH2 N D ER0 L AY1 N', 'AH1 N D ER0 L AY2 N', 'V\\\\\\n']\n",
      "['UNDERLINES', 'AH2 N D ER0 L AY1 N Z', 'AH1 N D ER0 L AY2 N Z', 'V\\\\\\n']\n",
      "['UNDERTAKING', 'AH2 N D ER0 T EY1 K IH0 NG', 'AH1 N D ER0 T EY2 K IH0 NG', 'V\\\\\\n']\n",
      "['UNDERTAKINGS', 'AH2 N D ER0 T EY1 K IH0 NG Z', 'AH1 N D ER0 T EY2 K IH0 NG Z', 'V\\\\\\n']\n",
      "['UNUSED', 'AH0 N Y UW1 Z D', 'AH0 N Y UW1 S T', 'V\\\\\\n']\n",
      "['UPGRADE', 'AH0 P G R EY1 D', 'AH1 P G R EY0 D', 'V\\\\\\n']\n",
      "['UPGRADES', 'AH0 P G R EY1 D Z', 'AH1 P G R EY0 D Z', 'V\\\\\\n']\n",
      "['UPLIFT', 'AH2 P L IH1 F T', 'AH1 P L IH0 F T', 'V\\\\\\n']\n",
      "['UPSET', 'AH0 P S EH1 T', 'AH1 P S EH2 T', 'V\\\\\\n']\n",
      "['UPSETS', 'AH0 P S EH1 T S', 'AH1 P S EH2 T S', 'V\\\\\\n']\n",
      "['USE', 'Y UW1 Z', 'Y UW1 S', 'V\\\\\\n']\n",
      "['USED', 'Y UW1 Z D', 'Y UW1 S T', 'VBN\\\\\\n']\n",
      "['USES', 'Y UW1 Z IH0 Z', 'Y UW1 S IH0 Z', 'V}']\n",
      "371\n"
     ]
    }
   ],
   "source": [
    "#G2P\n",
    "\n",
    "f = open(\"homograph.txt\")\n",
    "homograph = dict()\n",
    "for line in f.readlines()[10:]:\n",
    "  if line.startswith(\"#\"):continue\n",
    "  l = line.split(\"|\")\n",
    "  print(l)\n",
    "  homograph[l[0].lower()] = (l[1].split(), l[2].split(), l[3].replace(\"\\\\\\n\", \"\"))\n",
    "print(len(homograph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0321480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import cmudict\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "word_tokenize = TweetTokenizer().tokenize\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "from builtins import str as unicode\n",
    "\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger.zip')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "try:\n",
    "    nltk.data.find('corpora/cmudict.zip')\n",
    "except LookupError:\n",
    "    nltk.download('cmudict')\n",
    "\n",
    "\n",
    "\n",
    "def construct_homograph_dictionary():\n",
    "    homograph2features = dict()\n",
    "    f = open(\"homograph.txt\")\n",
    "    for line in f.readlines()[10:]:\n",
    "        #print(line)\n",
    "        if line.startswith(\"#\"): continue # comment\n",
    "        headword, pron1, pron2, pos1 = line.split(\"|\")[0], line.split(\"|\")[1],line.split(\"|\")[2],line.split(\"|\")[3].replace(\"\\\\\\n\", \"\")\n",
    "        homograph2features[headword.lower()] = (pron1.split(), pron2.split(), pos1)\n",
    "    return homograph2features\n",
    "\n",
    "\n",
    "class G2p(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.graphemes = [\"<pad>\", \"<unk>\", \"</s>\"] + list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "        self.phonemes = [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"] + ['AA0', 'AA1', 'AA2', 'AE0', 'AE1', 'AE2', 'AH0', 'AH1', 'AH2', 'AO0',\n",
    "                                                             'AO1', 'AO2', 'AW0', 'AW1', 'AW2', 'AY0', 'AY1', 'AY2', 'B', 'CH', 'D', 'DH',\n",
    "                                                             'EH0', 'EH1', 'EH2', 'ER0', 'ER1', 'ER2', 'EY0', 'EY1',\n",
    "                                                             'EY2', 'F', 'G', 'HH',\n",
    "                                                             'IH0', 'IH1', 'IH2', 'IY0', 'IY1', 'IY2', 'JH', 'K', 'L',\n",
    "                                                             'M', 'N', 'NG', 'OW0', 'OW1',\n",
    "                                                             'OW2', 'OY0', 'OY1', 'OY2', 'P', 'R', 'S', 'SH', 'T', 'TH',\n",
    "                                                             'UH0', 'UH1', 'UH2', 'UW',\n",
    "                                                             'UW0', 'UW1', 'UW2', 'V', 'W', 'Y', 'Z', 'ZH']\n",
    "        self.g2idx = {g: idx for idx, g in enumerate(self.graphemes)}\n",
    "        self.idx2g = {idx: g for idx, g in enumerate(self.graphemes)}\n",
    "\n",
    "        self.p2idx = {p: idx for idx, p in enumerate(self.phonemes)}\n",
    "        self.idx2p = {idx: p for idx, p in enumerate(self.phonemes)}\n",
    "\n",
    "        self.cmu = cmudict.dict()\n",
    "        self.load_variables()\n",
    "        self.homograph2features = construct_homograph_dictionary()\n",
    "\n",
    "    def load_variables(self):\n",
    "        self.variables = np.load(\"checkpoint20.npz\")\n",
    "        self.enc_emb = self.variables[\"enc_emb\"]  # (29, 64). (len(graphemes), emb)\n",
    "        self.enc_w_ih = self.variables[\"enc_w_ih\"]  # (3*128, 64)\n",
    "        self.enc_w_hh = self.variables[\"enc_w_hh\"]  # (3*128, 128)\n",
    "        self.enc_b_ih = self.variables[\"enc_b_ih\"]  # (3*128,)\n",
    "        self.enc_b_hh = self.variables[\"enc_b_hh\"]  # (3*128,)\n",
    "\n",
    "        self.dec_emb = self.variables[\"dec_emb\"]  # (74, 64). (len(phonemes), emb)\n",
    "        self.dec_w_ih = self.variables[\"dec_w_ih\"]  # (3*128, 64)\n",
    "        self.dec_w_hh = self.variables[\"dec_w_hh\"]  # (3*128, 128)\n",
    "        self.dec_b_ih = self.variables[\"dec_b_ih\"]  # (3*128,)\n",
    "        self.dec_b_hh = self.variables[\"dec_b_hh\"]  # (3*128,)\n",
    "        self.fc_w = self.variables[\"fc_w\"]  # (74, 128)\n",
    "        self.fc_b = self.variables[\"fc_b\"]  # (74,)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x)) # mathematical logistic function // for the activation of the neural network\n",
    "\n",
    "    def grucell(self, x, h, w_ih, w_hh, b_ih, b_hh):\n",
    "        rzn_ih = np.matmul(x, w_ih.T) + b_ih # matrix_product of two array\n",
    "        rzn_hh = np.matmul(h, w_hh.T) + b_hh\n",
    "\n",
    "        rz_ih, n_ih = rzn_ih[:, :rzn_ih.shape[-1] * 2 // 3], rzn_ih[:, rzn_ih.shape[-1] * 2 // 3:]\n",
    "        rz_hh, n_hh = rzn_hh[:, :rzn_hh.shape[-1] * 2 // 3], rzn_hh[:, rzn_hh.shape[-1] * 2 // 3:]\n",
    "\n",
    "        rz = self.sigmoid(rz_ih + rz_hh)\n",
    "        r, z = np.split(rz, 2, -1)\n",
    "\n",
    "        n = np.tanh(n_ih + r * n_hh)\n",
    "        h = (1 - z) * n + z * h\n",
    "\n",
    "        return h\n",
    "\n",
    "    def gru(self, x, steps, w_ih, w_hh, b_ih, b_hh, h0=None):\n",
    "        if h0 is None:\n",
    "            h0 = np.zeros((x.shape[0], w_hh.shape[1]), np.float32)\n",
    "        h = h0  # initial hidden state\n",
    "        outputs = np.zeros((x.shape[0], steps, w_hh.shape[1]), np.float32)\n",
    "        for t in range(steps):\n",
    "            h = self.grucell(x[:, t, :], h, w_ih, w_hh, b_ih, b_hh)  # (b, h)\n",
    "            outputs[:, t, ::] = h\n",
    "        return outputs\n",
    "\n",
    "    def encode(self, word):\n",
    "        chars = list(word) + [\"</s>\"]\n",
    "        x = [self.g2idx.get(char, self.g2idx[\"<unk>\"]) for char in chars]\n",
    "        x = np.take(self.enc_emb, np.expand_dims(x, 0), axis=0)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict(self, word):\n",
    "        # encoder\n",
    "        enc = self.encode(word)\n",
    "        enc = self.gru(enc, len(word) + 1, self.enc_w_ih, self.enc_w_hh,\n",
    "                       self.enc_b_ih, self.enc_b_hh, h0=np.zeros((1, self.enc_w_hh.shape[-1]), np.float32))\n",
    "        last_hidden = enc[:, -1, :]\n",
    "\n",
    "        # decoder\n",
    "        dec = np.take(self.dec_emb, [2], axis=0)  # 2: <s>\n",
    "        h = last_hidden\n",
    "\n",
    "        preds = []\n",
    "        for i in range(20):\n",
    "            h = self.grucell(dec, h, self.dec_w_ih, self.dec_w_hh, self.dec_b_ih, self.dec_b_hh)  # (b, h)\n",
    "            logits = np.matmul(h, self.fc_w.T) + self.fc_b\n",
    "            pred = logits.argmax()\n",
    "            if pred == 3: break  # 3: </s>\n",
    "            preds.append(pred)\n",
    "            dec = np.take(self.dec_emb, [pred], axis=0)\n",
    "\n",
    "        preds = [self.idx2p.get(idx, \"<unk>\") for idx in preds]\n",
    "        return preds\n",
    "\n",
    "    def __call__(self, text):\n",
    "        # preprocessing\n",
    "        text = unicode(text)\n",
    "        text = normalize_numbers(text)\n",
    "        text = ''.join(char for char in unicodedata.normalize('NFD', text) #Convert the text into its decomposed form\n",
    "                       if unicodedata.category(char) != 'Mn')  # Strip accents\n",
    "        text = text.lower()\n",
    "        text = re.sub(\"[^ a-z'.,?!\\-]\", \"\", text)\n",
    "        text = text.replace(\"i.e.\", \"that is\")\n",
    "        text = text.replace(\"e.g.\", \"for example\")\n",
    "\n",
    "        # tokenization\n",
    "        words = word_tokenize(text)\n",
    "        tokens = pos_tag(words)  # tuples of (word, tag)\n",
    "\n",
    "        # steps\n",
    "        prons = []\n",
    "        for word, pos in tokens:\n",
    "            if re.search(\"[a-z]\", word) is None:\n",
    "                pron = [word]\n",
    "\n",
    "            elif word in self.homograph2features:  # Check homograph\n",
    "                pron1, pron2, pos1 = self.homograph2features[word]\n",
    "                if pos.startswith(pos1):\n",
    "                    pron = pron1\n",
    "                else:\n",
    "                    pron = pron2\n",
    "            elif word in self.cmu:  # lookup CMU dict\n",
    "                pron = self.cmu[word][0]\n",
    "            else: # predict for oov\n",
    "                pron = self.predict(word)\n",
    "\n",
    "            prons.extend(pron)\n",
    "            prons.extend([\" \"])\n",
    "\n",
    "        return prons[:-1]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    texts = [\"I have $250 in my pocket.\", # number -> spell-out\n",
    "             \"popular pets, e.g. cats and dogs\", # e.g. -> for example\n",
    "             \"I refuse to collect the refuse around here.\", # homograph\n",
    "             \"I'm an activationist.\"] # newly coined word\n",
    "    g2p = G2p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b22a9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# gender_to_vector = defaultdict()\n",
    "# gender_to_vector_test = defaultdict()\n",
    "\n",
    "gender_to_vector = []\n",
    "gender_to_vector_test = []\n",
    "\n",
    "# name_to_idx = defaultdict()\n",
    "\n",
    "G2P_names = []\n",
    "G2P_names_test = []\n",
    "\n",
    "for i in range(X_test.size):\n",
    "    try:\n",
    "        name = ''.join(g2p(X_test[i]))\n",
    "        G2P_names_test.append(name)\n",
    "        \n",
    "        if y_test[i] == \"M\":\n",
    "            gender_to_vector_test.append(1)\n",
    "#             gender_to_vector_test[name] = 1\n",
    "        if y_test[i] == \"F\":\n",
    "            gender_to_vector_test.append(0)\n",
    "#             gender_to_vector_test[name] = 0\n",
    "    except KeyError:\n",
    "        continue \n",
    "\n",
    "\n",
    "for i in range(X_train.size):\n",
    "    try:\n",
    "        name = ''.join(g2p(X_train[i]))\n",
    "#         name_to_idx[name] = j\n",
    "        G2P_names.append(name)\n",
    "\n",
    "        if y_train[i] == \"M\":\n",
    "            gender_to_vector.append(1)\n",
    "#             gender_to_vector[name] = 1\n",
    "        if y_train[i] == \"F\":\n",
    "            gender_to_vector.append(0)\n",
    "#             gender_to_vector[name] = 0\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4710b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',daa0', ',daa0n', ',daa0ni', '0aa0$', '0aa0b', '0aa0be', '0aa0beh', '0aa0c', '0aa0ch', '0aa0che', '0aa0d', '0aa0da', '0aa0daa', '0aa0di', '0aa0dih', '0aa0diy', '0aa0do', '0aa0dow', '0aa0dr', '0aa0dri']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-24 {color: black;background-color: white;}#sk-container-id-24 pre{padding: 0;}#sk-container-id-24 div.sk-toggleable {background-color: white;}#sk-container-id-24 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-24 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-24 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-24 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-24 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-24 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-24 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-24 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-24 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-24 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-24 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-24 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-24 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-24 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-24 div.sk-item {position: relative;z-index: 1;}#sk-container-id-24 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-24 div.sk-item::before, #sk-container-id-24 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-24 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-24 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-24 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-24 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-24 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-24 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-24 div.sk-label-container {text-align: center;}#sk-container-id-24 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-24 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-24\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 CountVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(5, 7),\n",
       "                                 preprocessor=&lt;function &lt;lambda&gt; at 0x7fa33e469280&gt;)),\n",
       "                (&#x27;knn&#x27;, KNeighborsClassifier(n_neighbors=9))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-70\" type=\"checkbox\" ><label for=\"sk-estimator-id-70\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 CountVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(5, 7),\n",
       "                                 preprocessor=&lt;function &lt;lambda&gt; at 0x7fa33e469280&gt;)),\n",
       "                (&#x27;knn&#x27;, KNeighborsClassifier(n_neighbors=9))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-71\" type=\"checkbox\" ><label for=\"sk-estimator-id-71\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(5, 7),\n",
       "                preprocessor=&lt;function &lt;lambda&gt; at 0x7fa33e469280&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-72\" type=\"checkbox\" ><label for=\"sk-estimator-id-72\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=9)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 CountVectorizer(analyzer='char', ngram_range=(5, 7),\n",
       "                                 preprocessor=<function <lambda> at 0x7fa33e469280>)),\n",
       "                ('knn', KNeighborsClassifier(n_neighbors=9))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relevant sklearn import\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "# same as before\n",
    "vec = CountVectorizer(\n",
    "    min_df=1,\n",
    "    ngram_range=(5,7),\n",
    "    analyzer='char',\n",
    "    preprocessor=lambda x: f'^{x.lower().replace(\" \", \"\")}$',\n",
    ")\n",
    "# logistic regression with default parameters but more iterations\n",
    "# for the SGD to converge\n",
    "\n",
    "clf = LogisticRegression(max_iter=400)\n",
    "\n",
    "#using KNN to predict\n",
    "vec.fit(G2P_names)\n",
    "print(sorted(list(vec.vocabulary_))[:20])\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=9)\n",
    "\n",
    "# a pipeline to wrap sequential named steps\n",
    "# pipeline = Pipeline([('vectorizer', vec), ('logreg', clf)])\n",
    "pipeline = Pipeline([('vectorizer', vec), ('knn', neigh)])\n",
    "# build the vocabulary and train the model in a single command\n",
    "pipeline.fit(G2P_names, gender_to_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba248eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7905445840813884"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# predict gender of test names\n",
    "y_pred = pipeline.predict(G2P_names_test)\n",
    "# compute accuracy\n",
    "accuracy_score(gender_to_vector_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659111f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
